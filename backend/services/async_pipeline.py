"""
Async Processing Pipeline for OCR and Grading
æ™ºé˜…3.0å¼‚æ­¥å¤„ç†ç®¡é“å®ç°
"""

import asyncio
import json
import logging
import time
import uuid
from abc import ABC, abstractmethod
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List, Optional, Any, Callable, Set, Union, AsyncGenerator
from collections import defaultdict, deque
import aiofiles
import aiohttp
from pathlib import Path
import hashlib
import pickle
import concurrent.futures
import multiprocessing as mp
from contextlib import asynccontextmanager

logger = logging.getLogger(__name__)

class TaskStatus(str, Enum):
    """ä»»åŠ¡çŠ¶æ€"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    RETRYING = "retrying"
    PAUSED = "paused"

class TaskPriority(str, Enum):
    """ä»»åŠ¡ä¼˜å…ˆçº§"""
    LOW = "low"
    NORMAL = "normal"
    HIGH = "high"
    CRITICAL = "critical"

class PipelineStage(str, Enum):
    """ç®¡é“é˜¶æ®µ"""
    INPUT_VALIDATION = "input_validation"
    PREPROCESSING = "preprocessing"
    OCR_PROCESSING = "ocr_processing"
    POSTPROCESSING = "postprocessing"
    GRADING_ANALYSIS = "grading_analysis"
    RESULT_GENERATION = "result_generation"
    OUTPUT_DELIVERY = "output_delivery"

@dataclass
class TaskDefinition:
    """ä»»åŠ¡å®šä¹‰"""
    task_id: str
    task_type: str
    priority: TaskPriority = TaskPriority.NORMAL
    max_retries: int = 3
    timeout: int = 300  # 5åˆ†é’Ÿ
    depends_on: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # è¾“å…¥è¾“å‡ºå®šä¹‰
    input_schema: Dict[str, Any] = field(default_factory=dict)
    output_schema: Dict[str, Any] = field(default_factory=dict)
    
    # èµ„æºéœ€æ±‚
    cpu_requirement: float = 1.0  # CPUæ ¸å¿ƒæ•°
    memory_requirement: int = 512  # MB
    gpu_requirement: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

@dataclass
class TaskInstance:
    """ä»»åŠ¡å®ä¾‹"""
    instance_id: str
    task_def: TaskDefinition
    input_data: Dict[str, Any] = field(default_factory=dict)
    output_data: Dict[str, Any] = field(default_factory=dict)
    status: TaskStatus = TaskStatus.PENDING
    created_at: datetime = field(default_factory=datetime.now)
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    
    # æ‰§è¡Œä¿¡æ¯
    worker_id: Optional[str] = None
    retry_count: int = 0
    error_message: Optional[str] = None
    execution_log: List[str] = field(default_factory=list)
    
    # æ€§èƒ½æŒ‡æ ‡
    processing_time: float = 0.0
    resource_usage: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def duration(self) -> Optional[float]:
        """ä»»åŠ¡æ‰§è¡Œæ—¶é•¿"""
        if self.started_at and self.completed_at:
            return (self.completed_at - self.started_at).total_seconds()
        return None
    
    def add_log(self, message: str):
        """æ·»åŠ æ‰§è¡Œæ—¥å¿—"""
        timestamp = datetime.now().isoformat()
        self.execution_log.append(f"[{timestamp}] {message}")
        logger.info(f"Task {self.instance_id}: {message}")
        
    def to_dict(self) -> Dict[str, Any]:
        result = asdict(self)
        result['task_def'] = self.task_def.to_dict()
        return result

class TaskProcessor(ABC):
    """ä»»åŠ¡å¤„ç†å™¨æ¥å£"""
    
    @abstractmethod
    async def process(self, task: TaskInstance) -> Dict[str, Any]:
        """å¤„ç†ä»»åŠ¡"""
        pass
        
    @abstractmethod
    def get_processor_type(self) -> str:
        """è·å–å¤„ç†å™¨ç±»å‹"""
        pass
        
    @abstractmethod
    def validate_input(self, input_data: Dict[str, Any]) -> bool:
        """éªŒè¯è¾“å…¥æ•°æ®"""
        pass

class OCRProcessor(TaskProcessor):
    """OCRå¤„ç†å™¨"""
    
    def __init__(self):
        self.supported_formats = ['.jpg', '.jpeg', '.png', '.pdf', '.tiff']
        
    async def process(self, task: TaskInstance) -> Dict[str, Any]:
        """æ‰§è¡ŒOCRå¤„ç†"""
        task.add_log("Starting OCR processing")
        
        input_data = task.input_data
        file_path = input_data.get('file_path')
        
        if not file_path or not Path(file_path).exists():
            raise ValueError("Invalid file path")
            
        # æ¨¡æ‹ŸOCRå¤„ç†
        await asyncio.sleep(2)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        
        # æ¨¡æ‹ŸOCRç»“æœ
        ocr_result = {
            'text_content': f"Extracted text from {Path(file_path).name}",
            'confidence': 0.95,
            'regions': [
                {'text': 'Sample text', 'bbox': [100, 100, 200, 150], 'confidence': 0.98},
                {'text': 'Another text', 'bbox': [100, 200, 300, 250], 'confidence': 0.92}
            ],
            'language': 'zh-CN',
            'processing_time': 2.0
        }
        
        task.add_log(f"OCR completed with confidence {ocr_result['confidence']}")
        
        return {
            'ocr_result': ocr_result,
            'file_info': {
                'filename': Path(file_path).name,
                'size': Path(file_path).stat().st_size if Path(file_path).exists() else 0
            }
        }
        
    def get_processor_type(self) -> str:
        return "ocr_processor"
        
    def validate_input(self, input_data: Dict[str, Any]) -> bool:
        """éªŒè¯OCRè¾“å…¥"""
        file_path = input_data.get('file_path')
        if not file_path:
            return False
            
        path = Path(file_path)
        return path.exists() and path.suffix.lower() in self.supported_formats

class GradingProcessor(TaskProcessor):
    """é˜…å·å¤„ç†å™¨"""
    
    def __init__(self):
        self.grading_models = {
            'math': 'math_grading_model_v1',
            'chinese': 'chinese_grading_model_v1',
            'english': 'english_grading_model_v1'
        }
        
    async def process(self, task: TaskInstance) -> Dict[str, Any]:
        """æ‰§è¡Œé˜…å·å¤„ç†"""
        task.add_log("Starting grading analysis")
        
        input_data = task.input_data
        ocr_result = input_data.get('ocr_result', {})
        answer_key = input_data.get('answer_key', {})
        subject = input_data.get('subject', 'math')
        
        if not ocr_result or not answer_key:
            raise ValueError("Missing OCR result or answer key")
            
        # æ¨¡æ‹Ÿæ™ºèƒ½é˜…å·å¤„ç†
        await asyncio.sleep(3)  # æ¨¡æ‹Ÿåˆ†ææ—¶é—´
        
        # æ¨¡æ‹Ÿé˜…å·ç»“æœ
        grading_result = {
            'total_score': 85.5,
            'max_score': 100.0,
            'question_scores': [
                {'question_id': '1', 'score': 8.5, 'max_score': 10, 'feedback': 'è®¡ç®—æ­£ç¡®ï¼Œæ­¥éª¤æ¸…æ™°'},
                {'question_id': '2', 'score': 7.0, 'max_score': 8, 'feedback': 'æ–¹æ³•æ­£ç¡®ï¼Œä½†è®¡ç®—æœ‰å°é”™'},
                {'question_id': '3', 'score': 15.0, 'max_score': 15, 'feedback': 'å®Œå…¨æ­£ç¡®'}
            ],
            'grading_model': self.grading_models.get(subject, 'default_model'),
            'confidence': 0.89,
            'processing_time': 3.0,
            'analysis': {
                'strengths': ['è®¡ç®—èƒ½åŠ›å¼º', 'æ­¥éª¤æ¸…æ™°'],
                'weaknesses': ['ç»†èŠ‚æ³¨æ„ä¸å¤Ÿ', 'æ£€éªŒç¯èŠ‚ç¼ºå¤±'],
                'suggestions': ['åŠ å¼ºè®¡ç®—æ£€éªŒ', 'æ³¨æ„å•ä½æ¢ç®—']
            }
        }
        
        task.add_log(f"Grading completed: {grading_result['total_score']}/{grading_result['max_score']}")
        
        return {
            'grading_result': grading_result,
            'metadata': {
                'subject': subject,
                'model_version': self.grading_models.get(subject)
            }
        }
        
    def get_processor_type(self) -> str:
        return "grading_processor"
        
    def validate_input(self, input_data: Dict[str, Any]) -> bool:
        """éªŒè¯é˜…å·è¾“å…¥"""
        return bool(input_data.get('ocr_result')) and bool(input_data.get('answer_key'))

class PreprocessingProcessor(TaskProcessor):
    """é¢„å¤„ç†å™¨"""
    
    async def process(self, task: TaskInstance) -> Dict[str, Any]:
        """æ‰§è¡Œé¢„å¤„ç†"""
        task.add_log("Starting image preprocessing")
        
        input_data = task.input_data
        file_path = input_data.get('file_path')
        
        # æ¨¡æ‹Ÿå›¾åƒé¢„å¤„ç†
        await asyncio.sleep(1)
        
        preprocessing_result = {
            'processed_file_path': file_path,  # å®é™…ä¼šæ˜¯å¤„ç†åçš„æ–‡ä»¶è·¯å¾„
            'operations_applied': ['noise_reduction', 'rotation_correction', 'contrast_enhancement'],
            'quality_score': 0.92,
            'processing_time': 1.0
        }
        
        task.add_log("Preprocessing completed")
        
        return {'preprocessing_result': preprocessing_result}
        
    def get_processor_type(self) -> str:
        return "preprocessing_processor"
        
    def validate_input(self, input_data: Dict[str, Any]) -> bool:
        file_path = input_data.get('file_path')
        return bool(file_path) and Path(file_path).exists()

class WorkerPool:
    """å·¥ä½œæ± """
    
    def __init__(self, max_workers: int = 4, worker_type: str = "general"):
        self.max_workers = max_workers
        self.worker_type = worker_type
        self.active_workers: Dict[str, TaskInstance] = {}
        self.worker_stats: Dict[str, Dict[str, Any]] = defaultdict(lambda: {
            'tasks_completed': 0,
            'tasks_failed': 0,
            'total_processing_time': 0.0,
            'last_active': None
        })
        
    def is_available(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨å·¥ä½œè¿›ç¨‹"""
        return len(self.active_workers) < self.max_workers
        
    def assign_task(self, task: TaskInstance) -> str:
        """åˆ†é…ä»»åŠ¡"""
        if not self.is_available():
            raise RuntimeError("No available workers")
            
        worker_id = f"{self.worker_type}_worker_{len(self.active_workers)}"
        self.active_workers[worker_id] = task
        task.worker_id = worker_id
        
        return worker_id
        
    def complete_task(self, worker_id: str, success: bool, processing_time: float):
        """å®Œæˆä»»åŠ¡"""
        if worker_id in self.active_workers:
            del self.active_workers[worker_id]
            
        stats = self.worker_stats[worker_id]
        if success:
            stats['tasks_completed'] += 1
        else:
            stats['tasks_failed'] += 1
        stats['total_processing_time'] += processing_time
        stats['last_active'] = datetime.now()
        
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å·¥ä½œæ± ç»Ÿè®¡"""
        return {
            'worker_type': self.worker_type,
            'max_workers': self.max_workers,
            'active_workers': len(self.active_workers),
            'available_workers': self.max_workers - len(self.active_workers),
            'worker_stats': dict(self.worker_stats)
        }

class PipelineOrchestrator:
    """ç®¡é“ç¼–æ’å™¨"""
    
    def __init__(self):
        self.processors: Dict[str, TaskProcessor] = {}
        self.worker_pools: Dict[str, WorkerPool] = {}
        self.task_queue: Dict[TaskPriority, deque] = {
            priority: deque() for priority in TaskPriority
        }
        self.running_tasks: Dict[str, TaskInstance] = {}
        self.completed_tasks: Dict[str, TaskInstance] = {}
        self.pipeline_definitions: Dict[str, List[PipelineStage]] = {}
        
        self.running = False
        self.scheduler_task: Optional[asyncio.Task] = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_tasks': 0,
            'completed_tasks': 0,
            'failed_tasks': 0,
            'average_processing_time': 0.0
        }
        
    def register_processor(self, processor: TaskProcessor):
        """æ³¨å†Œå¤„ç†å™¨"""
        processor_type = processor.get_processor_type()
        self.processors[processor_type] = processor
        
        # ä¸ºæ¯ç§å¤„ç†å™¨ç±»å‹åˆ›å»ºå·¥ä½œæ± 
        if processor_type not in self.worker_pools:
            self.worker_pools[processor_type] = WorkerPool(max_workers=4, worker_type=processor_type)
            
        logger.info(f"Registered processor: {processor_type}")
        
    def define_pipeline(self, pipeline_name: str, stages: List[PipelineStage]):
        """å®šä¹‰å¤„ç†ç®¡é“"""
        self.pipeline_definitions[pipeline_name] = stages
        logger.info(f"Defined pipeline: {pipeline_name} with {len(stages)} stages")
        
    async def start(self):
        """å¯åŠ¨ç®¡é“ç¼–æ’å™¨"""
        if self.running:
            return
            
        self.running = True
        self.scheduler_task = asyncio.create_task(self._scheduler_loop())
        
        logger.info("Pipeline orchestrator started")
        
    async def stop(self):
        """åœæ­¢ç®¡é“ç¼–æ’å™¨"""
        if not self.running:
            return
            
        self.running = False
        
        if self.scheduler_task:
            self.scheduler_task.cancel()
            try:
                await self.scheduler_task
            except asyncio.CancelledError:
                pass
                
        # ç­‰å¾…æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡å®Œæˆ
        if self.running_tasks:
            logger.info(f"Waiting for {len(self.running_tasks)} running tasks to complete...")
            await asyncio.gather(*[
                self._wait_for_task(task) for task in self.running_tasks.values()
            ], return_exceptions=True)
            
        logger.info("Pipeline orchestrator stopped")
        
    async def submit_task(self, task_def: TaskDefinition, input_data: Dict[str, Any]) -> str:
        """æäº¤ä»»åŠ¡"""
        instance_id = str(uuid.uuid4())
        
        task_instance = TaskInstance(
            instance_id=instance_id,
            task_def=task_def,
            input_data=input_data
        )
        
        # éªŒè¯è¾“å…¥
        processor = self.processors.get(task_def.task_type)
        if not processor:
            raise ValueError(f"No processor found for task type: {task_def.task_type}")
            
        if not processor.validate_input(input_data):
            raise ValueError("Input validation failed")
            
        # åŠ å…¥é˜Ÿåˆ—
        self.task_queue[task_def.priority].append(task_instance)
        self.stats['total_tasks'] += 1
        
        logger.info(f"Submitted task: {instance_id} ({task_def.task_type})")
        
        return instance_id
        
    async def submit_pipeline(self, pipeline_name: str, input_data: Dict[str, Any]) -> List[str]:
        """æäº¤ç®¡é“ä»»åŠ¡"""
        if pipeline_name not in self.pipeline_definitions:
            raise ValueError(f"Pipeline not defined: {pipeline_name}")
            
        stages = self.pipeline_definitions[pipeline_name]
        task_ids = []
        previous_task_id = None
        
        for stage in stages:
            # æ ¹æ®é˜¶æ®µåˆ›å»ºä»»åŠ¡å®šä¹‰
            task_def = TaskDefinition(
                task_id=str(uuid.uuid4()),
                task_type=self._stage_to_processor_type(stage),
                priority=TaskPriority.NORMAL,
                depends_on=[previous_task_id] if previous_task_id else []
            )
            
            task_id = await self.submit_task(task_def, input_data)
            task_ids.append(task_id)
            previous_task_id = task_id
            
        logger.info(f"Submitted pipeline: {pipeline_name} with {len(task_ids)} tasks")
        
        return task_ids
        
    def _stage_to_processor_type(self, stage: PipelineStage) -> str:
        """å°†ç®¡é“é˜¶æ®µæ˜ å°„åˆ°å¤„ç†å™¨ç±»å‹"""
        mapping = {
            PipelineStage.PREPROCESSING: "preprocessing_processor",
            PipelineStage.OCR_PROCESSING: "ocr_processor",
            PipelineStage.GRADING_ANALYSIS: "grading_processor"
        }
        return mapping.get(stage, "general_processor")
        
    async def get_task_status(self, task_id: str) -> Optional[TaskInstance]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        # æ£€æŸ¥æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
        if task_id in self.running_tasks:
            return self.running_tasks[task_id]
            
        # æ£€æŸ¥å·²å®Œæˆçš„ä»»åŠ¡
        if task_id in self.completed_tasks:
            return self.completed_tasks[task_id]
            
        # æ£€æŸ¥é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡
        for priority_queue in self.task_queue.values():
            for task in priority_queue:
                if task.instance_id == task_id:
                    return task
                    
        return None
        
    async def cancel_task(self, task_id: str) -> bool:
        """å–æ¶ˆä»»åŠ¡"""
        # ä»é˜Ÿåˆ—ä¸­ç§»é™¤
        for priority_queue in self.task_queue.values():
            for i, task in enumerate(priority_queue):
                if task.instance_id == task_id:
                    task.status = TaskStatus.CANCELLED
                    del priority_queue[i]
                    logger.info(f"Cancelled queued task: {task_id}")
                    return True
                    
        # å–æ¶ˆæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
        if task_id in self.running_tasks:
            task = self.running_tasks[task_id]
            task.status = TaskStatus.CANCELLED
            # è¿™é‡Œåº”è¯¥å®ç°ä»»åŠ¡ä¸­æ–­é€»è¾‘
            logger.info(f"Cancelled running task: {task_id}")
            return True
            
        return False
        
    async def _scheduler_loop(self):
        """è°ƒåº¦å™¨å¾ªç¯"""
        while self.running:
            try:
                await self._schedule_tasks()
                await asyncio.sleep(0.1)  # è°ƒåº¦é—´éš”
            except Exception as e:
                logger.error(f"Scheduler error: {str(e)}")
                await asyncio.sleep(1)
                
    async def _schedule_tasks(self):
        """è°ƒåº¦ä»»åŠ¡"""
        # æŒ‰ä¼˜å…ˆçº§å¤„ç†ä»»åŠ¡
        for priority in [TaskPriority.CRITICAL, TaskPriority.HIGH, TaskPriority.NORMAL, TaskPriority.LOW]:
            queue = self.task_queue[priority]
            
            while queue:
                task = queue[0]  # æŸ¥çœ‹é˜Ÿåˆ—å¤´éƒ¨
                
                # æ£€æŸ¥ä¾èµ–
                if not self._check_dependencies(task):
                    break  # ä¾èµ–æœªæ»¡è¶³ï¼Œè·³å‡ºå½“å‰ä¼˜å…ˆçº§
                    
                # æ£€æŸ¥èµ„æº
                processor_type = task.task_def.task_type
                worker_pool = self.worker_pools.get(processor_type)
                
                if not worker_pool or not worker_pool.is_available():
                    break  # èµ„æºä¸è¶³
                    
                # å¼€å§‹æ‰§è¡Œä»»åŠ¡
                queue.popleft()
                await self._execute_task(task)
                
    def _check_dependencies(self, task: TaskInstance) -> bool:
        """æ£€æŸ¥ä»»åŠ¡ä¾èµ–"""
        for dep_task_id in task.task_def.depends_on:
            dep_task = self.completed_tasks.get(dep_task_id)
            if not dep_task or dep_task.status != TaskStatus.COMPLETED:
                return False
        return True
        
    async def _execute_task(self, task: TaskInstance):
        """æ‰§è¡Œä»»åŠ¡"""
        processor_type = task.task_def.task_type
        processor = self.processors[processor_type]
        worker_pool = self.worker_pools[processor_type]
        
        # åˆ†é…å·¥ä½œè¿›ç¨‹
        worker_id = worker_pool.assign_task(task)
        
        task.status = TaskStatus.RUNNING
        task.started_at = datetime.now()
        self.running_tasks[task.instance_id] = task
        
        # åˆ›å»ºæ‰§è¡Œä»»åŠ¡
        execution_task = asyncio.create_task(
            self._run_task(task, processor, worker_pool)
        )
        
        task.add_log(f"Task assigned to worker {worker_id}")
        
    async def _run_task(self, task: TaskInstance, processor: TaskProcessor, worker_pool: WorkerPool):
        """è¿è¡Œä»»åŠ¡"""
        start_time = time.time()
        success = False
        
        try:
            # è®¾ç½®è¶…æ—¶
            result = await asyncio.wait_for(
                processor.process(task),
                timeout=task.task_def.timeout
            )
            
            task.output_data = result
            task.status = TaskStatus.COMPLETED
            task.completed_at = datetime.now()
            success = True
            
            task.add_log("Task completed successfully")
            
        except asyncio.TimeoutError:
            task.status = TaskStatus.FAILED
            task.error_message = "Task timeout"
            task.add_log("Task failed: timeout")
            
        except Exception as e:
            task.status = TaskStatus.FAILED
            task.error_message = str(e)
            task.add_log(f"Task failed: {str(e)}")
            
            # é‡è¯•é€»è¾‘
            if task.retry_count < task.task_def.max_retries:
                task.retry_count += 1
                task.status = TaskStatus.RETRYING
                task.add_log(f"Retrying task (attempt {task.retry_count})")
                
                # é‡æ–°å…¥é˜Ÿ
                await asyncio.sleep(2 ** task.retry_count)  # æŒ‡æ•°é€€é¿
                self.task_queue[task.task_def.priority].appendleft(task)
                
        finally:
            processing_time = time.time() - start_time
            task.processing_time = processing_time
            
            # æ›´æ–°å·¥ä½œæ± ç»Ÿè®¡
            worker_pool.complete_task(task.worker_id, success, processing_time)
            
            # ä»è¿è¡Œä»»åŠ¡ä¸­ç§»é™¤
            if task.instance_id in self.running_tasks:
                del self.running_tasks[task.instance_id]
                
            # æ·»åŠ åˆ°å·²å®Œæˆä»»åŠ¡
            if task.status in [TaskStatus.COMPLETED, TaskStatus.FAILED]:
                self.completed_tasks[task.instance_id] = task
                
                if success:
                    self.stats['completed_tasks'] += 1
                else:
                    self.stats['failed_tasks'] += 1
                    
                # æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
                if self.stats['completed_tasks'] > 0:
                    total_time = self.stats['average_processing_time'] * (self.stats['completed_tasks'] - 1) + processing_time
                    self.stats['average_processing_time'] = total_time / self.stats['completed_tasks']
                    
    async def _wait_for_task(self, task: TaskInstance):
        """ç­‰å¾…ä»»åŠ¡å®Œæˆ"""
        while task.status in [TaskStatus.PENDING, TaskStatus.RUNNING, TaskStatus.RETRYING]:
            await asyncio.sleep(0.1)
            
    def get_pipeline_stats(self) -> Dict[str, Any]:
        """è·å–ç®¡é“ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'timestamp': datetime.now().isoformat(),
            'orchestrator_stats': self.stats.copy(),
            'queue_lengths': {
                priority.value: len(queue) for priority, queue in self.task_queue.items()
            },
            'running_tasks': len(self.running_tasks),
            'completed_tasks': len(self.completed_tasks),
            'worker_pools': {
                name: pool.get_stats() for name, pool in self.worker_pools.items()
            }
        }

# ä½¿ç”¨ç¤ºä¾‹
async def demo_async_pipeline():
    """å¼‚æ­¥ç®¡é“æ¼”ç¤º"""
    print("ğŸš€ Async Pipeline Demo Starting...")
    
    orchestrator = PipelineOrchestrator()
    
    # æ³¨å†Œå¤„ç†å™¨
    orchestrator.register_processor(PreprocessingProcessor())
    orchestrator.register_processor(OCRProcessor())
    orchestrator.register_processor(GradingProcessor())
    
    # å®šä¹‰å®Œæ•´å¤„ç†ç®¡é“
    orchestrator.define_pipeline("complete_grading_pipeline", [
        PipelineStage.PREPROCESSING,
        PipelineStage.OCR_PROCESSING,
        PipelineStage.GRADING_ANALYSIS
    ])
    
    await orchestrator.start()
    
    try:
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        test_file = Path("./test_exam.jpg")
        test_file.touch()  # åˆ›å»ºç©ºæ–‡ä»¶ç”¨äºæµ‹è¯•
        
        # æäº¤å•ä¸ªOCRä»»åŠ¡
        ocr_task_def = TaskDefinition(
            task_id="ocr_001",
            task_type="ocr_processor",
            priority=TaskPriority.HIGH
        )
        
        ocr_task_id = await orchestrator.submit_task(ocr_task_def, {
            'file_path': str(test_file)
        })
        
        print(f"ğŸ“ Submitted OCR task: {ocr_task_id}")
        
        # æäº¤å®Œæ•´ç®¡é“
        pipeline_input = {
            'file_path': str(test_file),
            'subject': 'math',
            'answer_key': {'q1': 'A', 'q2': 'B', 'q3': 'C'}
        }
        
        pipeline_task_ids = await orchestrator.submit_pipeline("complete_grading_pipeline", pipeline_input)
        print(f"ğŸ“‹ Submitted pipeline tasks: {pipeline_task_ids}")
        
        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        await asyncio.sleep(10)
        
        # è·å–ä»»åŠ¡çŠ¶æ€
        for task_id in [ocr_task_id] + pipeline_task_ids:
            task_status = await orchestrator.get_task_status(task_id)
            if task_status:
                print(f"ğŸ“Š Task {task_id}: {task_status.status.value}")
                if task_status.status == TaskStatus.COMPLETED:
                    print(f"    Duration: {task_status.duration:.2f}s")
                    
        # è·å–ç®¡é“ç»Ÿè®¡
        stats = orchestrator.get_pipeline_stats()
        print(f"ğŸ“ˆ Pipeline Stats:\n{json.dumps(stats, indent=2, default=str)}")
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        if test_file.exists():
            test_file.unlink()
            
    finally:
        await orchestrator.stop()
        
    print("âœ… Async Pipeline Demo Completed!")

if __name__ == "__main__":
    asyncio.run(demo_async_pipeline())