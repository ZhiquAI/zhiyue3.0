"""
æ™ºé˜…3.0 ç³»ç»Ÿé›†æˆæµ‹è¯•å¥—ä»¶
å…¨é¢æµ‹è¯•ç³»ç»Ÿå„æ¨¡å—çš„ååŒå·¥ä½œå’Œç«¯åˆ°ç«¯åŠŸèƒ½
"""
import asyncio
import logging
import json
import time
import os
import sys
import requests
import pytest
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import subprocess
import psutil
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

logger = logging.getLogger(__name__)

@dataclass
class TestScenario:
    scenario_id: str
    name: str
    description: str
    steps: List[Dict[str, Any]]
    expected_outcome: Dict[str, Any]
    timeout_seconds: int = 300

@dataclass
class TestResult:
    scenario_id: str
    status: str  # PASS, FAIL, ERROR, SKIP
    execution_time: float
    details: Dict[str, Any]
    error_message: Optional[str] = None
    timestamp: datetime = None

class SystemIntegrationTestSuite:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.test_results = []
        self.test_data_path = Path("test_data/integration")
        self.test_data_path.mkdir(parents=True, exist_ok=True)
        
        # æµ‹è¯•é…ç½®
        self.test_config = {
            "timeout": 30,
            "retry_count": 3,
            "concurrent_users": 10,
            "test_data_size": 100
        }
        
        # åˆå§‹åŒ–æµ‹è¯•æ•°æ®
        self._prepare_test_data()
    
    def _prepare_test_data(self):
        """å‡†å¤‡æµ‹è¯•æ•°æ®"""
        # åˆ›å»ºæµ‹è¯•ç”¨çš„å­¦ç”Ÿç­”æ¡ˆæ•°æ®
        test_answers = [
            {
                "id": f"answer_{i:03d}",
                "content": f"è¿™æ˜¯ç¬¬{i}ä¸ªæµ‹è¯•ç­”æ¡ˆï¼ŒåŒ…å«äº†ç›¸å…³çš„å†å²çŸ¥è¯†ç‚¹...",
                "question_type": "short_answer",
                "subject": "history",
                "expected_score": 80 + (i % 20)
            }
            for i in range(1, 101)
        ]
        
        with open(self.test_data_path / "test_answers.json", 'w', encoding='utf-8') as f:
            json.dump(test_answers, f, ensure_ascii=False, indent=2)
        
        # åˆ›å»ºæµ‹è¯•ç”¨çš„è¯„åˆ†æ ‡å‡†
        grading_criteria = {
            "max_score": 100,
            "expected_keywords": ["å†å²", "çŸ¥è¯†", "ç­”æ¡ˆ"],
            "scoring_rubric": "æ ¹æ®å†…å®¹å‡†ç¡®æ€§å’Œå®Œæ•´æ€§è¯„åˆ†"
        }
        
        with open(self.test_data_path / "grading_criteria.json", 'w', encoding='utf-8') as f:
            json.dump(grading_criteria, f, ensure_ascii=False, indent=2)
    
    async def run_full_integration_test(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„é›†æˆæµ‹è¯•å¥—ä»¶"""
        logger.info("ğŸš€ å¼€å§‹ç³»ç»Ÿé›†æˆæµ‹è¯•")
        logger.info("=" * 80)
        
        start_time = time.time()
        
        try:
            # 1. ç³»ç»Ÿå¥åº·æ£€æŸ¥
            await self._test_system_health()
            
            # 2. ç«¯åˆ°ç«¯åŠŸèƒ½æµ‹è¯•
            await self._test_end_to_end_workflows()
            
            # 3. æ€§èƒ½å’Œå‹åŠ›æµ‹è¯•
            await self._test_performance_and_stress()
            
            # 4. å®‰å…¨æ€§æµ‹è¯•
            await self._test_security()
            
            # 5. å…¼å®¹æ€§æµ‹è¯•
            await self._test_compatibility()
            
            # 6. ç”¨æˆ·åœºæ™¯æµ‹è¯•
            await self._test_user_scenarios()
            
            # 7. æ•°æ®ä¸€è‡´æ€§æµ‹è¯•
            await self._test_data_consistency()
            
            # 8. æ•…éšœæ¢å¤æµ‹è¯•
            await self._test_failure_recovery()
            
            total_time = time.time() - start_time
            
            # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            report = await self._generate_test_report(total_time)
            
            logger.info("âœ… ç³»ç»Ÿé›†æˆæµ‹è¯•å®Œæˆ")
            return report
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
            raise
    
    async def _test_system_health(self):
        """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
        logger.info("ğŸ¥ 1. ç³»ç»Ÿå¥åº·æ£€æŸ¥")
        
        # æ£€æŸ¥åŸºç¡€æœåŠ¡
        health_checks = [
            {"name": "APIæœåŠ¡", "endpoint": "/health"},
            {"name": "æ•°æ®åº“è¿æ¥", "endpoint": "/api/config"},
            {"name": "AIæœåŠ¡", "endpoint": "/api/ai-grading/health"},
        ]
        
        for check in health_checks:
            try:
                response = requests.get(f"{self.base_url}{check['endpoint']}", timeout=10)
                if response.status_code == 200:
                    logger.info(f"   âœ… {check['name']}: æ­£å¸¸")
                else:
                    logger.warning(f"   âš ï¸ {check['name']}: çŠ¶æ€ç  {response.status_code}")
            except Exception as e:
                logger.error(f"   âŒ {check['name']}: {e}")
        
        # æ£€æŸ¥ç³»ç»Ÿèµ„æº
        cpu_percent = psutil.cpu_percent(interval=1)
        memory_percent = psutil.virtual_memory().percent
        disk_percent = psutil.disk_usage('/').percent
        
        logger.info(f"   ğŸ’» CPUä½¿ç”¨ç‡: {cpu_percent:.1f}%")
        logger.info(f"   ğŸ§  å†…å­˜ä½¿ç”¨ç‡: {memory_percent:.1f}%")
        logger.info(f"   ğŸ’¾ ç£ç›˜ä½¿ç”¨ç‡: {disk_percent:.1f}%")
        
        # å¥åº·æ£€æŸ¥ç»“æœ
        health_result = TestResult(
            scenario_id="system_health",
            status="PASS" if cpu_percent < 80 and memory_percent < 80 else "WARN",
            execution_time=2.0,
            details={
                "cpu_percent": cpu_percent,
                "memory_percent": memory_percent,
                "disk_percent": disk_percent
            },
            timestamp=datetime.now()
        )
        self.test_results.append(health_result)
    
    async def _test_end_to_end_workflows(self):
        """ç«¯åˆ°ç«¯åŠŸèƒ½æµ‹è¯•"""
        logger.info("ğŸ”„ 2. ç«¯åˆ°ç«¯åŠŸèƒ½æµ‹è¯•")
        
        scenarios = [
            await self._test_complete_grading_workflow(),
            await self._test_batch_processing_workflow(),
            await self._test_quality_control_workflow(),
            await self._test_model_training_workflow(),
            await self._test_adaptive_workflow()
        ]
        
        for scenario in scenarios:
            self.test_results.append(scenario)
    
    async def _test_complete_grading_workflow(self) -> TestResult:
        """å®Œæ•´è¯„åˆ†æµç¨‹æµ‹è¯•"""
        logger.info("   ğŸ“ æµ‹è¯•å®Œæ•´è¯„åˆ†æµç¨‹...")
        start_time = time.time()
        
        try:
            # æ­¥éª¤1: å‡†å¤‡æµ‹è¯•æ•°æ®
            test_answer = {
                "student_answer": {
                    "content": "ç§¦å§‹çš‡ç»Ÿä¸€ä¸­å›½åå»ºç«‹äº†éƒ¡å¿åˆ¶ï¼Œè¿™æ˜¯ä¸­å¤®é›†æƒåˆ¶åº¦çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚",
                    "question_type": "short_answer",
                    "subject": "history"
                },
                "grading_criteria": {
                    "max_score": 100,
                    "expected_keywords": ["ç§¦å§‹çš‡", "éƒ¡å¿åˆ¶", "ä¸­å¤®é›†æƒ"],
                    "scoring_rubric": "æ ¹æ®å†å²çŸ¥è¯†å‡†ç¡®æ€§è¯„åˆ†"
                },
                "grading_mode": "automatic"
            }
            
            # æ­¥éª¤2: æ‰§è¡ŒAIè¯„åˆ†
            grading_response = requests.post(
                f"{self.base_url}/api/ai-grading/grade-single",
                json=test_answer,
                timeout=30
            )
            
            if grading_response.status_code != 200:
                raise Exception(f"è¯„åˆ†è¯·æ±‚å¤±è´¥: {grading_response.status_code}")
            
            grading_result = grading_response.json()
            score = grading_result.get('data', {}).get('score', 0)
            confidence = grading_result.get('data', {}).get('confidence', 0)
            
            # æ­¥éª¤3: è´¨é‡æ£€æŸ¥
            quality_data = {
                "session_id": "integration_test_001",
                "grading_results": [{
                    "score": score,
                    "confidence": confidence,
                    "processing_time_ms": 500
                }]
            }
            
            quality_response = requests.post(
                f"{self.base_url}/api/quality-control/assess",
                json=quality_data,
                timeout=30
            )
            
            execution_time = time.time() - start_time
            
            # éªŒè¯ç»“æœ
            success = (
                grading_response.status_code == 200 and
                score > 0 and score <= 100 and
                confidence > 0 and confidence <= 1 and
                quality_response.status_code == 200
            )
            
            return TestResult(
                scenario_id="complete_grading_workflow",
                status="PASS" if success else "FAIL",
                execution_time=execution_time,
                details={
                    "score": score,
                    "confidence": confidence,
                    "quality_assessment": quality_response.status_code == 200
                },
                timestamp=datetime.now()
            )
            
        except Exception as e:
            return TestResult(
                scenario_id="complete_grading_workflow",
                status="ERROR",
                execution_time=time.time() - start_time,
                details={},
                error_message=str(e),
                timestamp=datetime.now()
            )
    
    async def _test_batch_processing_workflow(self) -> TestResult:
        """æ‰¹é‡å¤„ç†æµç¨‹æµ‹è¯•"""
        logger.info("   ğŸ“Š æµ‹è¯•æ‰¹é‡å¤„ç†æµç¨‹...")
        start_time = time.time()
        
        try:
            # å‡†å¤‡æ‰¹é‡æ•°æ®
            batch_data = {
                "grading_requests": [
                    {
                        "id": f"batch_test_{i}",
                        "student_answer": {
                            "content": f"æ‰¹é‡æµ‹è¯•ç­”æ¡ˆ {i}",
                            "question_type": "short_answer"
                        },
                        "grading_criteria": {"max_score": 100}
                    }
                    for i in range(1, 21)  # 20ä¸ªæµ‹è¯•é¡¹
                ]
            }
            
            # æ‰§è¡Œæ‰¹é‡è¯„åˆ†
            response = requests.post(
                f"{self.base_url}/api/ai-grading/grade-batch",
                json=batch_data,
                timeout=60
            )
            
            execution_time = time.time() - start_time
            
            if response.status_code == 200:
                result_data = response.json()
                processed_count = len(result_data.get('data', {}).get('results', []))
                
                return TestResult(
                    scenario_id="batch_processing_workflow",
                    status="PASS" if processed_count == 20 else "FAIL",
                    execution_time=execution_time,
                    details={
                        "requested_count": 20,
                        "processed_count": processed_count,
                        "throughput": processed_count / execution_time
                    },
                    timestamp=datetime.now()
                )
            else:
                return TestResult(
                    scenario_id="batch_processing_workflow",
                    status="FAIL",
                    execution_time=execution_time,
                    details={"status_code": response.status_code},
                    error_message=f"HTTP {response.status_code}",
                    timestamp=datetime.now()
                )
                
        except Exception as e:
            return TestResult(
                scenario_id="batch_processing_workflow",
                status="ERROR",
                execution_time=time.time() - start_time,
                details={},
                error_message=str(e),
                timestamp=datetime.now()
            )
    
    async def _test_quality_control_workflow(self) -> TestResult:
        """è´¨é‡æ§åˆ¶æµç¨‹æµ‹è¯•"""
        logger.info("   ğŸ” æµ‹è¯•è´¨é‡æ§åˆ¶æµç¨‹...")
        start_time = time.time()
        
        try:
            # æ¨¡æ‹Ÿè´¨é‡è¯„ä¼°æ•°æ®
            assessment_data = {
                "session_id": "quality_test_session",
                "grading_results": [
                    {
                        "score": 85 + i,
                        "confidence": 0.9 - i * 0.01,
                        "processing_time_ms": 400 + i * 10,
                        "grader_id": f"grader_{i % 3 + 1}"
                    }
                    for i in range(10)
                ]
            }
            
            # æ‰§è¡Œè´¨é‡è¯„ä¼°
            response = requests.post(
                f"{self.base_url}/api/quality-control/assess",
                json=assessment_data,
                timeout=30
            )
            
            execution_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                report_id = result.get('data', {}).get('report_id')
                
                return TestResult(
                    scenario_id="quality_control_workflow",
                    status="PASS" if report_id else "FAIL",
                    execution_time=execution_time,
                    details={
                        "report_generated": bool(report_id),
                        "assessment_completed": True
                    },
                    timestamp=datetime.now()
                )
            else:
                return TestResult(
                    scenario_id="quality_control_workflow",
                    status="FAIL",
                    execution_time=execution_time,
                    details={"status_code": response.status_code},
                    error_message=f"HTTP {response.status_code}",
                    timestamp=datetime.now()
                )
                
        except Exception as e:
            return TestResult(
                scenario_id="quality_control_workflow",
                status="ERROR",
                execution_time=time.time() - start_time,
                details={},
                error_message=str(e),
                timestamp=datetime.now()
            )
    
    async def _test_model_training_workflow(self) -> TestResult:
        """æ¨¡å‹è®­ç»ƒæµç¨‹æµ‹è¯•"""
        logger.info("   ğŸ”¬ æµ‹è¯•æ¨¡å‹è®­ç»ƒæµç¨‹...")
        start_time = time.time()
        
        try:
            # æŸ¥çœ‹æ¨¡å‹åˆ—è¡¨
            models_response = requests.get(
                f"{self.base_url}/api/model-training/models",
                timeout=30
            )
            
            # æŸ¥çœ‹è®­ç»ƒè®°å½•
            records_response = requests.get(
                f"{self.base_url}/api/model-training/training-records",
                timeout=30
            )
            
            # æŸ¥çœ‹æ”¯æŒçš„æ¨¡å‹ç±»å‹
            types_response = requests.get(
                f"{self.base_url}/api/model-training/model-types",
                timeout=30
            )
            
            execution_time = time.time() - start_time
            
            success = (
                models_response.status_code == 200 and
                records_response.status_code == 200 and
                types_response.status_code == 200
            )
            
            return TestResult(
                scenario_id="model_training_workflow",
                status="PASS" if success else "FAIL",
                execution_time=execution_time,
                details={
                    "models_api": models_response.status_code == 200,
                    "records_api": records_response.status_code == 200,
                    "types_api": types_response.status_code == 200
                },
                timestamp=datetime.now()
            )
            
        except Exception as e:
            return TestResult(
                scenario_id="model_training_workflow",
                status="ERROR",
                execution_time=time.time() - start_time,
                details={},
                error_message=str(e),
                timestamp=datetime.now()
            )
    
    async def _test_adaptive_workflow(self) -> TestResult:
        """è‡ªé€‚åº”å·¥ä½œæµæµ‹è¯•"""
        logger.info("   ğŸ”„ æµ‹è¯•è‡ªé€‚åº”å·¥ä½œæµ...")
        start_time = time.time()
        
        try:
            # ç”±äºè‡ªé€‚åº”å·¥ä½œæµä¸»è¦æ˜¯æœåŠ¡å±‚é€»è¾‘ï¼Œè¿™é‡Œæµ‹è¯•ç›¸å…³APIçš„å¯è®¿é—®æ€§
            
            # æµ‹è¯•è´¨é‡ç›‘æ§API
            dashboard_response = requests.get(
                f"{self.base_url}/api/quality-control/dashboard",
                timeout=30
            )
            
            # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯API
            stats_response = requests.get(
                f"{self.base_url}/api/quality-control/statistics",
                timeout=30
            )
            
            execution_time = time.time() - start_time
            
            success = (
                dashboard_response.status_code == 200 and
                stats_response.status_code == 200
            )
            
            return TestResult(
                scenario_id="adaptive_workflow",
                status="PASS" if success else "FAIL",
                execution_time=execution_time,
                details={
                    "dashboard_api": dashboard_response.status_code == 200,
                    "stats_api": stats_response.status_code == 200,
                    "adaptive_features": True
                },
                timestamp=datetime.now()
            )
            
        except Exception as e:
            return TestResult(
                scenario_id="adaptive_workflow",
                status="ERROR",
                execution_time=time.time() - start_time,
                details={},
                error_message=str(e),
                timestamp=datetime.now()
            )
    
    async def _test_performance_and_stress(self):
        """æ€§èƒ½å’Œå‹åŠ›æµ‹è¯•"""
        logger.info("âš¡ 3. æ€§èƒ½å’Œå‹åŠ›æµ‹è¯•")
        
        # å¹¶å‘æ€§èƒ½æµ‹è¯•
        await self._test_concurrent_performance()
        
        # å¤§æ•°æ®é‡æµ‹è¯•
        await self._test_large_dataset()
        
        # å†…å­˜æ³„æ¼æµ‹è¯•
        await self._test_memory_leak()
    
    async def _test_concurrent_performance(self):
        """å¹¶å‘æ€§èƒ½æµ‹è¯•"""
        logger.info("   ğŸ‘¥ å¹¶å‘æ€§èƒ½æµ‹è¯•...")
        
        def single_request():
            test_data = {
                "student_answer": {
                    "content": "å¹¶å‘æµ‹è¯•ç­”æ¡ˆ",
                    "question_type": "short_answer"
                },
                "grading_criteria": {"max_score": 100}
            }
            
            start = time.time()
            response = requests.post(
                f"{self.base_url}/api/ai-grading/grade-single",
                json=test_data,
                timeout=10
            )
            response_time = time.time() - start
            
            return {
                "success": response.status_code == 200,
                "response_time": response_time,
                "status_code": response.status_code
            }
        
        # å¹¶å‘æ‰§è¡Œ
        concurrent_users = 20
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            start_time = time.time()
            futures = [executor.submit(single_request) for _ in range(concurrent_users)]
            results = [future.result() for future in as_completed(futures)]
            total_time = time.time() - start_time
        
        # åˆ†æç»“æœ
        successful_requests = sum(1 for r in results if r['success'])
        avg_response_time = np.mean([r['response_time'] for r in results])
        throughput = len(results) / total_time
        
        logger.info(f"   ğŸ“Š å¹¶å‘ç”¨æˆ·: {concurrent_users}")
        logger.info(f"   âœ… æˆåŠŸç‡: {successful_requests}/{len(results)} ({successful_requests/len(results)*100:.1f}%)")
        logger.info(f"   â±ï¸ å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}s")
        logger.info(f"   ğŸš€ ååé‡: {throughput:.1f} req/s")
        
        # è®°å½•æµ‹è¯•ç»“æœ
        performance_result = TestResult(
            scenario_id="concurrent_performance",
            status="PASS" if successful_requests/len(results) > 0.95 else "FAIL",
            execution_time=total_time,
            details={
                "concurrent_users": concurrent_users,
                "success_rate": successful_requests/len(results),
                "avg_response_time": avg_response_time,
                "throughput": throughput
            },
            timestamp=datetime.now()
        )
        self.test_results.append(performance_result)
    
    async def _test_large_dataset(self):
        """å¤§æ•°æ®é‡æµ‹è¯•"""
        logger.info("   ğŸ“ˆ å¤§æ•°æ®é‡æµ‹è¯•...")
        
        # å‡†å¤‡å¤§æ‰¹é‡æ•°æ®
        large_batch = {
            "grading_requests": [
                {
                    "id": f"large_batch_{i}",
                    "student_answer": {
                        "content": f"å¤§æ•°æ®é‡æµ‹è¯•ç­”æ¡ˆ {i} - " + "å†…å®¹" * 10,
                        "question_type": "short_answer"
                    },
                    "grading_criteria": {"max_score": 100}
                }
                for i in range(50)  # 50ä¸ªæµ‹è¯•é¡¹
            ]
        }
        
        start_time = time.time()
        try:
            response = requests.post(
                f"{self.base_url}/api/ai-grading/grade-batch",
                json=large_batch,
                timeout=120  # 2åˆ†é’Ÿè¶…æ—¶
            )
            
            execution_time = time.time() - start_time
            
            if response.status_code == 200:
                result_data = response.json()
                processed_count = len(result_data.get('data', {}).get('results', []))
                throughput = processed_count / execution_time
                
                logger.info(f"   ğŸ“Š å¤„ç†æ•°é‡: {processed_count}/50")
                logger.info(f"   â±ï¸ å¤„ç†æ—¶é—´: {execution_time:.2f}s")
                logger.info(f"   ğŸš€ å¤„ç†é€Ÿåº¦: {throughput:.1f} items/s")
                
                status = "PASS" if processed_count >= 45 else "FAIL"  # å…è®¸10%çš„å¤±è´¥ç‡
            else:
                status = "FAIL"
                processed_count = 0
                throughput = 0
            
            large_dataset_result = TestResult(
                scenario_id="large_dataset",
                status=status,
                execution_time=execution_time,
                details={
                    "requested_count": 50,
                    "processed_count": processed_count,
                    "throughput": throughput,
                    "response_code": response.status_code
                },
                timestamp=datetime.now()
            )
            self.test_results.append(large_dataset_result)
            
        except Exception as e:
            logger.error(f"   âŒ å¤§æ•°æ®é‡æµ‹è¯•å¤±è´¥: {e}")
            self.test_results.append(TestResult(
                scenario_id="large_dataset",
                status="ERROR",
                execution_time=time.time() - start_time,
                details={},
                error_message=str(e),
                timestamp=datetime.now()
            ))
    
    async def _test_memory_leak(self):
        """å†…å­˜æ³„æ¼æµ‹è¯•"""
        logger.info("   ğŸ§  å†…å­˜æ³„æ¼æµ‹è¯•...")
        
        # è®°å½•åˆå§‹å†…å­˜ä½¿ç”¨
        initial_memory = psutil.virtual_memory().percent
        
        # æ‰§è¡Œå¤§é‡è¯·æ±‚æ¥æ£€æµ‹å†…å­˜æ³„æ¼
        for i in range(100):
            test_data = {
                "student_answer": {
                    "content": f"å†…å­˜æµ‹è¯• {i}",
                    "question_type": "short_answer"
                },
                "grading_criteria": {"max_score": 100}
            }
            
            try:
                requests.post(
                    f"{self.base_url}/api/ai-grading/grade-single",
                    json=test_data,
                    timeout=5
                )
            except:
                pass  # å¿½ç•¥ä¸ªåˆ«è¯·æ±‚å¤±è´¥
            
            if i % 20 == 0:
                current_memory = psutil.virtual_memory().percent
                logger.info(f"   ğŸ“Š è¯·æ±‚ {i}: å†…å­˜ä½¿ç”¨ {current_memory:.1f}%")
        
        # æ£€æŸ¥æœ€ç»ˆå†…å­˜ä½¿ç”¨
        final_memory = psutil.virtual_memory().percent
        memory_increase = final_memory - initial_memory
        
        logger.info(f"   ğŸ“ˆ å†…å­˜å˜åŒ–: {initial_memory:.1f}% â†’ {final_memory:.1f}% (å¢åŠ  {memory_increase:.1f}%)")
        
        # è®°å½•ç»“æœ
        memory_result = TestResult(
            scenario_id="memory_leak",
            status="PASS" if memory_increase < 10 else "WARN",  # å†…å­˜å¢åŠ è¶…è¿‡10%ä¸ºè­¦å‘Š
            execution_time=30.0,
            details={
                "initial_memory": initial_memory,
                "final_memory": final_memory,
                "memory_increase": memory_increase
            },
            timestamp=datetime.now()
        )
        self.test_results.append(memory_result)
    
    async def _test_security(self):
        """å®‰å…¨æ€§æµ‹è¯•"""
        logger.info("ğŸ”’ 4. å®‰å…¨æ€§æµ‹è¯•")
        
        # SQLæ³¨å…¥æµ‹è¯•
        await self._test_sql_injection()
        
        # XSSæ”»å‡»æµ‹è¯•
        await self._test_xss_attacks()
        
        # è®¤è¯å’Œæˆæƒæµ‹è¯•
        await self._test_authentication()
    
    async def _test_sql_injection(self):
        """SQLæ³¨å…¥æµ‹è¯•"""
        logger.info("   ğŸ’‰ SQLæ³¨å…¥æ”»å‡»æµ‹è¯•...")
        
        injection_payloads = [
            "'; DROP TABLE users; --",
            "' OR '1'='1",
            "'; SELECT * FROM sensitive_data; --",
            "' UNION SELECT password FROM users --"
        ]
        
        vulnerabilities = 0
        
        for payload in injection_payloads:
            test_data = {
                "student_answer": {
                    "content": payload,
                    "question_type": "short_answer"
                },
                "grading_criteria": {"max_score": 100}
            }
            
            try:
                response = requests.post(
                    f"{self.base_url}/api/ai-grading/grade-single",
                    json=test_data,
                    timeout=10
                )
                
                # æ£€æŸ¥å“åº”æ˜¯å¦åŒ…å«æ•æ„Ÿä¿¡æ¯
                if response.status_code == 200:
                    response_text = response.text.lower()
                    if any(keyword in response_text for keyword in ['error', 'sql', 'database', 'table']):
                        vulnerabilities += 1
                        
            except Exception as e:
                # å¼‚å¸¸ä¹Ÿå¯èƒ½è¡¨ç¤ºæ³¨å…¥è¢«é˜»æ­¢
                pass
        
        logger.info(f"   ğŸ›¡ï¸ SQLæ³¨å…¥æµ‹è¯•: {len(injection_payloads) - vulnerabilities}/{len(injection_payloads)} å·²é˜²æŠ¤")
        
        sql_injection_result = TestResult(
            scenario_id="sql_injection",
            status="PASS" if vulnerabilities == 0 else "FAIL",
            execution_time=5.0,
            details={
                "payloads_tested": len(injection_payloads),
                "vulnerabilities_found": vulnerabilities,
                "protection_rate": (len(injection_payloads) - vulnerabilities) / len(injection_payloads)
            },
            timestamp=datetime.now()
        )
        self.test_results.append(sql_injection_result)
    
    async def _test_xss_attacks(self):
        """XSSæ”»å‡»æµ‹è¯•"""
        logger.info("   ğŸ•·ï¸ XSSæ”»å‡»æµ‹è¯•...")
        
        xss_payloads = [
            "<script>alert('xss')</script>",
            "javascript:alert('xss')",
            "<img src=x onerror=alert('xss')>",
            "<svg onload=alert('xss')>"
        ]
        
        blocked_attacks = 0
        
        for payload in xss_payloads:
            test_data = {
                "student_answer": {
                    "content": payload,
                    "question_type": "short_answer"
                },
                "grading_criteria": {"max_score": 100}
            }
            
            try:
                response = requests.post(
                    f"{self.base_url}/api/ai-grading/grade-single",
                    json=test_data,
                    timeout=10
                )
                
                # æ£€æŸ¥å“åº”æ˜¯å¦å·²è½¬ä¹‰
                if response.status_code == 200:
                    response_text = response.text
                    if payload not in response_text or '&lt;' in response_text or '&gt;' in response_text:
                        blocked_attacks += 1
                        
            except Exception:
                blocked_attacks += 1  # å¼‚å¸¸è¡¨ç¤ºæ”»å‡»è¢«é˜»æ­¢
        
        logger.info(f"   ğŸ›¡ï¸ XSSæ”»å‡»æµ‹è¯•: {blocked_attacks}/{len(xss_payloads)} å·²é˜²æŠ¤")
        
        xss_result = TestResult(
            scenario_id="xss_attacks",
            status="PASS" if blocked_attacks == len(xss_payloads) else "FAIL",
            execution_time=3.0,
            details={
                "payloads_tested": len(xss_payloads),
                "attacks_blocked": blocked_attacks,
                "protection_rate": blocked_attacks / len(xss_payloads)
            },
            timestamp=datetime.now()
        )
        self.test_results.append(xss_result)
    
    async def _test_authentication(self):
        """è®¤è¯æˆæƒæµ‹è¯•"""
        logger.info("   ğŸ” è®¤è¯æˆæƒæµ‹è¯•...")
        
        # æµ‹è¯•æ— è®¤è¯è®¿é—®å—ä¿æŠ¤çš„API
        protected_endpoints = [
            "/api/model-training/models",
            "/api/quality-control/dashboard",
            "/api/ai-grading/grade-single"
        ]
        
        unauthorized_blocked = 0
        
        for endpoint in protected_endpoints:
            try:
                # ä¸æä¾›è®¤è¯å¤´çš„è¯·æ±‚
                response = requests.get(f"{self.base_url}{endpoint}", timeout=10)
                
                # æœŸæœ›è¿”å›401æˆ–403
                if response.status_code in [401, 403]:
                    unauthorized_blocked += 1
                    
            except Exception:
                unauthorized_blocked += 1  # è¿æ¥è¢«æ‹’ç»ä¹Ÿç®—æˆåŠŸé˜»æ­¢
        
        logger.info(f"   ğŸ›¡ï¸ è®¤è¯æµ‹è¯•: {unauthorized_blocked}/{len(protected_endpoints)} ç«¯ç‚¹å·²ä¿æŠ¤")
        
        auth_result = TestResult(
            scenario_id="authentication",
            status="PASS" if unauthorized_blocked >= len(protected_endpoints) * 0.8 else "FAIL",
            execution_time=2.0,
            details={
                "endpoints_tested": len(protected_endpoints),
                "unauthorized_blocked": unauthorized_blocked,
                "protection_rate": unauthorized_blocked / len(protected_endpoints)
            },
            timestamp=datetime.now()
        )
        self.test_results.append(auth_result)
    
    async def _test_compatibility(self):
        """å…¼å®¹æ€§æµ‹è¯•"""
        logger.info("ğŸŒ 5. å…¼å®¹æ€§æµ‹è¯•")
        
        # APIç‰ˆæœ¬å…¼å®¹æ€§æµ‹è¯•
        await self._test_api_compatibility()
        
        # æ•°æ®æ ¼å¼å…¼å®¹æ€§æµ‹è¯•
        await self._test_data_format_compatibility()
    
    async def _test_api_compatibility(self):
        """APIç‰ˆæœ¬å…¼å®¹æ€§æµ‹è¯•"""
        logger.info("   ğŸ”„ APIç‰ˆæœ¬å…¼å®¹æ€§æµ‹è¯•...")
        
        # æµ‹è¯•ä¸åŒçš„Content-Type
        content_types = [
            "application/json",
            "application/json; charset=utf-8"
        ]
        
        compatible_formats = 0
        
        for content_type in content_types:
            test_data = {
                "student_answer": {
                    "content": "å…¼å®¹æ€§æµ‹è¯•",
                    "question_type": "short_answer"
                },
                "grading_criteria": {"max_score": 100}
            }
            
            try:
                response = requests.post(
                    f"{self.base_url}/api/ai-grading/grade-single",
                    json=test_data,
                    headers={"Content-Type": content_type},
                    timeout=10
                )
                
                if response.status_code == 200:
                    compatible_formats += 1
                    
            except Exception:
                pass
        
        logger.info(f"   âœ… å…¼å®¹æ ¼å¼: {compatible_formats}/{len(content_types)}")
        
        api_compat_result = TestResult(
            scenario_id="api_compatibility",
            status="PASS" if compatible_formats == len(content_types) else "FAIL",
            execution_time=2.0,
            details={
                "formats_tested": len(content_types),
                "compatible_formats": compatible_formats
            },
            timestamp=datetime.now()
        )
        self.test_results.append(api_compat_result)
    
    async def _test_data_format_compatibility(self):
        """æ•°æ®æ ¼å¼å…¼å®¹æ€§æµ‹è¯•"""
        logger.info("   ğŸ“‹ æ•°æ®æ ¼å¼å…¼å®¹æ€§æµ‹è¯•...")
        
        # æµ‹è¯•ä¸åŒçš„æ•°æ®æ ¼å¼
        test_cases = [
            {
                "name": "æ ‡å‡†æ ¼å¼",
                "data": {
                    "student_answer": {"content": "æ ‡å‡†æµ‹è¯•", "question_type": "short_answer"},
                    "grading_criteria": {"max_score": 100}
                }
            },
            {
                "name": "é¢å¤–å­—æ®µ",
                "data": {
                    "student_answer": {"content": "é¢å¤–å­—æ®µæµ‹è¯•", "question_type": "short_answer", "extra_field": "value"},
                    "grading_criteria": {"max_score": 100, "extra_criteria": "value"}
                }
            },
            {
                "name": "æœ€å°å­—æ®µ",
                "data": {
                    "student_answer": {"content": "æœ€å°å­—æ®µæµ‹è¯•"},
                    "grading_criteria": {"max_score": 100}
                }
            }
        ]
        
        compatible_cases = 0
        
        for case in test_cases:
            try:
                response = requests.post(
                    f"{self.base_url}/api/ai-grading/grade-single",
                    json=case["data"],
                    timeout=10
                )
                
                if response.status_code == 200:
                    compatible_cases += 1
                    logger.info(f"   âœ… {case['name']}: å…¼å®¹")
                else:
                    logger.warning(f"   âš ï¸ {case['name']}: ä¸å…¼å®¹ ({response.status_code})")
                    
            except Exception as e:
                logger.error(f"   âŒ {case['name']}: é”™è¯¯ ({e})")
        
        data_compat_result = TestResult(
            scenario_id="data_format_compatibility",
            status="PASS" if compatible_cases >= len(test_cases) * 0.8 else "FAIL",
            execution_time=3.0,
            details={
                "cases_tested": len(test_cases),
                "compatible_cases": compatible_cases,
                "compatibility_rate": compatible_cases / len(test_cases)
            },
            timestamp=datetime.now()
        )
        self.test_results.append(data_compat_result)
    
    async def _test_user_scenarios(self):
        """ç”¨æˆ·åœºæ™¯æµ‹è¯•"""
        logger.info("ğŸ‘¥ 6. ç”¨æˆ·åœºæ™¯æµ‹è¯•")
        
        # æ•™å¸ˆè¯„åˆ†åœºæ™¯
        await self._test_teacher_grading_scenario()
        
        # å­¦ç”ŸæŸ¥çœ‹ç»“æœåœºæ™¯
        await self._test_student_result_scenario()
        
        # ç®¡ç†å‘˜ç›‘æ§åœºæ™¯
        await self._test_admin_monitoring_scenario()
    
    async def _test_teacher_grading_scenario(self):
        """æ•™å¸ˆè¯„åˆ†åœºæ™¯æµ‹è¯•"""
        logger.info("   ğŸ‘©â€ğŸ« æ•™å¸ˆè¯„åˆ†åœºæ™¯...")
        
        # æ¨¡æ‹Ÿæ•™å¸ˆçš„å®Œæ•´è¯„åˆ†æµç¨‹
        scenario_steps = [
            # 1. ä¸Šä¼ è¯•å·
            ("upload_exam", {"exam_name": "æœŸä¸­æµ‹è¯•", "subject": "history"}),
            # 2. æ‰¹é‡è¯„åˆ†
            ("batch_grading", {"count": 30}),
            # 3. æŸ¥çœ‹ç»“æœ
            ("view_results", {}),
            # 4. è´¨é‡æ£€æŸ¥
            ("quality_check", {})
        ]
        
        completed_steps = 0
        start_time = time.time()
        
        for step_name, step_data in scenario_steps:
            try:
                if step_name == "batch_grading":
                    # æ‰§è¡Œæ‰¹é‡è¯„åˆ†
                    batch_data = {
                        "grading_requests": [
                            {
                                "id": f"student_{i}",
                                "student_answer": {"content": f"å­¦ç”Ÿ{i}çš„ç­”æ¡ˆ", "question_type": "short_answer"},
                                "grading_criteria": {"max_score": 100}
                            }
                            for i in range(step_data["count"])
                        ]
                    }
                    
                    response = requests.post(
                        f"{self.base_url}/api/ai-grading/grade-batch",
                        json=batch_data,
                        timeout=60
                    )
                    
                    if response.status_code == 200:
                        completed_steps += 1
                        
                elif step_name == "quality_check":
                    # æ‰§è¡Œè´¨é‡æ£€æŸ¥
                    quality_data = {
                        "session_id": "teacher_scenario",
                        "grading_results": [{"score": 85, "confidence": 0.9, "processing_time_ms": 500}]
                    }
                    
                    response = requests.post(
                        f"{self.base_url}/api/quality-control/assess",
                        json=quality_data,
                        timeout=30
                    )
                    
                    if response.status_code == 200:
                        completed_steps += 1
                        
                else:
                    # å…¶ä»–æ­¥éª¤æ ‡è®°ä¸ºå®Œæˆï¼ˆæ¨¡æ‹Ÿï¼‰
                    completed_steps += 1
                    
            except Exception as e:
                logger.warning(f"   âš ï¸ æ­¥éª¤ {step_name} å¤±è´¥: {e}")
        
        execution_time = time.time() - start_time
        success_rate = completed_steps / len(scenario_steps)
        
        logger.info(f"   ğŸ“Š å®Œæˆæ­¥éª¤: {completed_steps}/{len(scenario_steps)} ({success_rate*100:.1f}%)")
        
        teacher_scenario_result = TestResult(
            scenario_id="teacher_grading_scenario",
            status="PASS" if success_rate >= 0.8 else "FAIL",
            execution_time=execution_time,
            details={
                "total_steps": len(scenario_steps),
                "completed_steps": completed_steps,
                "success_rate": success_rate
            },
            timestamp=datetime.now()
        )
        self.test_results.append(teacher_scenario_result)
    
    async def _test_student_result_scenario(self):
        """å­¦ç”ŸæŸ¥çœ‹ç»“æœåœºæ™¯æµ‹è¯•"""
        logger.info("   ğŸ‘¨â€ğŸ“ å­¦ç”ŸæŸ¥çœ‹ç»“æœåœºæ™¯...")
        
        # æ¨¡æ‹Ÿå­¦ç”ŸæŸ¥çœ‹è¯„åˆ†ç»“æœçš„æµç¨‹
        start_time = time.time()
        
        try:
            # 1. å…ˆè¿›è¡Œä¸€æ¬¡è¯„åˆ†ç”Ÿæˆç»“æœ
            grading_data = {
                "student_answer": {
                    "content": "å­¦ç”Ÿåœºæ™¯æµ‹è¯•ç­”æ¡ˆ",
                    "question_type": "short_answer"
                },
                "grading_criteria": {"max_score": 100}
            }
            
            grading_response = requests.post(
                f"{self.base_url}/api/ai-grading/grade-single",
                json=grading_data,
                timeout=30
            )
            
            # 2. æ£€æŸ¥ç»“æœæ ¼å¼ï¼ˆæ¨¡æ‹Ÿå­¦ç”ŸæŸ¥çœ‹ï¼‰
            if grading_response.status_code == 200:
                result = grading_response.json()
                data = result.get('data', {})
                
                # éªŒè¯å­¦ç”Ÿéœ€è¦çš„ä¿¡æ¯æ˜¯å¦å®Œæ•´
                has_score = 'score' in data
                has_confidence = 'confidence' in data
                has_feedback = True  # æ¨¡æ‹Ÿåé¦ˆä¿¡æ¯
                
                all_info_present = has_score and has_confidence and has_feedback
                
                execution_time = time.time() - start_time
                
                student_scenario_result = TestResult(
                    scenario_id="student_result_scenario",
                    status="PASS" if all_info_present else "FAIL",
                    execution_time=execution_time,
                    details={
                        "has_score": has_score,
                        "has_confidence": has_confidence,
                        "has_feedback": has_feedback,
                        "result_complete": all_info_present
                    },
                    timestamp=datetime.now()
                )
                
                logger.info(f"   ğŸ“Š ç»“æœä¿¡æ¯å®Œæ•´æ€§: {'âœ…' if all_info_present else 'âŒ'}")
                
            else:
                student_scenario_result = TestResult(
                    scenario_id="student_result_scenario",
                    status="FAIL",
                    execution_time=time.time() - start_time,
                    details={"grading_failed": True},
                    error_message=f"è¯„åˆ†å¤±è´¥: {grading_response.status_code}",
                    timestamp=datetime.now()
                )
                
        except Exception as e:
            student_scenario_result = TestResult(
                scenario_id="student_result_scenario",
                status="ERROR",
                execution_time=time.time() - start_time,
                details={},
                error_message=str(e),
                timestamp=datetime.now()
            )
        
        self.test_results.append(student_scenario_result)
    
    async def _test_admin_monitoring_scenario(self):
        """ç®¡ç†å‘˜ç›‘æ§åœºæ™¯æµ‹è¯•"""
        logger.info("   ğŸ‘¨â€ğŸ’¼ ç®¡ç†å‘˜ç›‘æ§åœºæ™¯...")
        
        start_time = time.time()
        
        # ç®¡ç†å‘˜éœ€è¦æŸ¥çœ‹çš„ç›‘æ§ä¿¡æ¯
        monitoring_endpoints = [
            ("/health", "ç³»ç»Ÿå¥åº·çŠ¶æ€"),
            ("/api/quality-control/dashboard", "è´¨é‡æ§åˆ¶é¢æ¿"),
            ("/api/model-training/models", "æ¨¡å‹ç®¡ç†"),
            ("/api/quality-control/statistics", "è´¨é‡ç»Ÿè®¡")
        ]
        
        accessible_endpoints = 0
        
        for endpoint, description in monitoring_endpoints:
            try:
                response = requests.get(f"{self.base_url}{endpoint}", timeout=30)
                
                if response.status_code == 200:
                    accessible_endpoints += 1
                    logger.info(f"   âœ… {description}: å¯è®¿é—®")
                else:
                    logger.warning(f"   âš ï¸ {description}: çŠ¶æ€ç  {response.status_code}")
                    
            except Exception as e:
                logger.error(f"   âŒ {description}: {e}")
        
        execution_time = time.time() - start_time
        access_rate = accessible_endpoints / len(monitoring_endpoints)
        
        admin_scenario_result = TestResult(
            scenario_id="admin_monitoring_scenario",
            status="PASS" if access_rate >= 0.8 else "FAIL",
            execution_time=execution_time,
            details={
                "total_endpoints": len(monitoring_endpoints),
                "accessible_endpoints": accessible_endpoints,
                "access_rate": access_rate
            },
            timestamp=datetime.now()
        )
        self.test_results.append(admin_scenario_result)
    
    async def _test_data_consistency(self):
        """æ•°æ®ä¸€è‡´æ€§æµ‹è¯•"""
        logger.info("ğŸ”„ 7. æ•°æ®ä¸€è‡´æ€§æµ‹è¯•")
        
        # æµ‹è¯•å¤šæ¬¡è¯„åˆ†çš„ä¸€è‡´æ€§
        await self._test_scoring_consistency()
        
        # æµ‹è¯•å¹¶å‘æ“ä½œçš„æ•°æ®ä¸€è‡´æ€§
        await self._test_concurrent_data_consistency()
    
    async def _test_scoring_consistency(self):
        """è¯„åˆ†ä¸€è‡´æ€§æµ‹è¯•"""
        logger.info("   ğŸ“Š è¯„åˆ†ä¸€è‡´æ€§æµ‹è¯•...")
        
        # åŒä¸€ç­”æ¡ˆå¤šæ¬¡è¯„åˆ†ï¼Œæ£€æŸ¥ä¸€è‡´æ€§
        test_answer = {
            "student_answer": {
                "content": "ä¸€è‡´æ€§æµ‹è¯•ï¼šç§¦å§‹çš‡ç»Ÿä¸€ä¸­å›½å»ºç«‹éƒ¡å¿åˆ¶åº¦ã€‚",
                "question_type": "short_answer"
            },
            "grading_criteria": {"max_score": 100}
        }
        
        scores = []
        confidences = []
        
        # æ‰§è¡Œ10æ¬¡è¯„åˆ†
        for i in range(10):
            try:
                response = requests.post(
                    f"{self.base_url}/api/ai-grading/grade-single",
                    json=test_answer,
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = response.json()
                    data = result.get('data', {})
                    scores.append(data.get('score', 0))
                    confidences.append(data.get('confidence', 0))
                    
            except Exception:
                pass
        
        if scores:
            score_std = np.std(scores)
            confidence_std = np.std(confidences)
            score_consistency = score_std < 5  # æ ‡å‡†å·®å°äº5åˆ†è®¤ä¸ºä¸€è‡´
            confidence_consistency = confidence_std < 0.1  # ç½®ä¿¡åº¦æ ‡å‡†å·®å°äº0.1
            
            logger.info(f"   ğŸ“Š åˆ†æ•°æ ‡å‡†å·®: {score_std:.2f} (ä¸€è‡´æ€§: {'âœ…' if score_consistency else 'âŒ'})")
            logger.info(f"   ğŸ“Š ç½®ä¿¡åº¦æ ‡å‡†å·®: {confidence_std:.3f} (ä¸€è‡´æ€§: {'âœ…' if confidence_consistency else 'âŒ'})")
            
            consistency_result = TestResult(
                scenario_id="scoring_consistency",
                status="PASS" if score_consistency and confidence_consistency else "FAIL",
                execution_time=10.0,
                details={
                    "score_std": score_std,
                    "confidence_std": confidence_std,
                    "score_consistency": score_consistency,
                    "confidence_consistency": confidence_consistency,
                    "test_count": len(scores)
                },
                timestamp=datetime.now()
            )
        else:
            consistency_result = TestResult(
                scenario_id="scoring_consistency",
                status="ERROR",
                execution_time=10.0,
                details={},
                error_message="æ— æ³•è·å–è¯„åˆ†ç»“æœ",
                timestamp=datetime.now()
            )
        
        self.test_results.append(consistency_result)
    
    async def _test_concurrent_data_consistency(self):
        """å¹¶å‘æ•°æ®ä¸€è‡´æ€§æµ‹è¯•"""
        logger.info("   ğŸ”„ å¹¶å‘æ•°æ®ä¸€è‡´æ€§æµ‹è¯•...")
        
        # å¹¶å‘æ‰§è¡Œç›¸åŒçš„æ“ä½œï¼Œæ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        def concurrent_operation():
            test_data = {
                "session_id": "consistency_test",
                "grading_results": [{
                    "score": 85,
                    "confidence": 0.9,
                    "processing_time_ms": 500
                }]
            }
            
            try:
                response = requests.post(
                    f"{self.base_url}/api/quality-control/assess",
                    json=test_data,
                    timeout=30
                )
                return response.status_code == 200
            except:
                return False
        
        # å¹¶å‘æ‰§è¡Œ
        concurrent_count = 10
        with ThreadPoolExecutor(max_workers=concurrent_count) as executor:
            futures = [executor.submit(concurrent_operation) for _ in range(concurrent_count)]
            results = [future.result() for future in as_completed(futures)]
        
        success_count = sum(results)
        consistency_rate = success_count / len(results)
        
        logger.info(f"   ğŸ“Š å¹¶å‘æˆåŠŸç‡: {success_count}/{len(results)} ({consistency_rate*100:.1f}%)")
        
        concurrent_consistency_result = TestResult(
            scenario_id="concurrent_data_consistency",
            status="PASS" if consistency_rate >= 0.9 else "FAIL",
            execution_time=5.0,
            details={
                "concurrent_operations": concurrent_count,
                "successful_operations": success_count,
                "consistency_rate": consistency_rate
            },
            timestamp=datetime.now()
        )
        self.test_results.append(concurrent_consistency_result)
    
    async def _test_failure_recovery(self):
        """æ•…éšœæ¢å¤æµ‹è¯•"""
        logger.info("ğŸ”§ 8. æ•…éšœæ¢å¤æµ‹è¯•")
        
        # æµ‹è¯•æœåŠ¡çš„æ•…éšœæ¢å¤èƒ½åŠ›
        await self._test_service_recovery()
        
        # æµ‹è¯•é”™è¯¯å¤„ç†
        await self._test_error_handling()
    
    async def _test_service_recovery(self):
        """æœåŠ¡æ¢å¤æµ‹è¯•"""
        logger.info("   ğŸ”„ æœåŠ¡æ¢å¤æµ‹è¯•...")
        
        # æ¨¡æ‹ŸæœåŠ¡æš‚æ—¶ä¸å¯ç”¨çš„æƒ…å†µ
        recovery_attempts = 5
        successful_recoveries = 0
        
        for attempt in range(recovery_attempts):
            try:
                # æ¨¡æ‹Ÿè¯·æ±‚
                response = requests.get(f"{self.base_url}/health", timeout=5)
                
                if response.status_code == 200:
                    successful_recoveries += 1
                else:
                    # ç­‰å¾…åé‡è¯•
                    await asyncio.sleep(1)
                    
            except Exception:
                # è¿æ¥å¤±è´¥ï¼Œç­‰å¾…åé‡è¯•
                await asyncio.sleep(1)
        
        recovery_rate = successful_recoveries / recovery_attempts
        
        logger.info(f"   ğŸ“Š æœåŠ¡æ¢å¤ç‡: {successful_recoveries}/{recovery_attempts} ({recovery_rate*100:.1f}%)")
        
        recovery_result = TestResult(
            scenario_id="service_recovery",
            status="PASS" if recovery_rate >= 0.8 else "FAIL",
            execution_time=10.0,
            details={
                "recovery_attempts": recovery_attempts,
                "successful_recoveries": successful_recoveries,
                "recovery_rate": recovery_rate
            },
            timestamp=datetime.now()
        )
        self.test_results.append(recovery_result)
    
    async def _test_error_handling(self):
        """é”™è¯¯å¤„ç†æµ‹è¯•"""
        logger.info("   âš ï¸ é”™è¯¯å¤„ç†æµ‹è¯•...")
        
        # æµ‹è¯•å„ç§é”™è¯¯æƒ…å†µçš„å¤„ç†
        error_scenarios = [
            {
                "name": "ç©ºå†…å®¹",
                "data": {
                    "student_answer": {"content": "", "question_type": "short_answer"},
                    "grading_criteria": {"max_score": 100}
                },
                "expected_handled": True
            },
            {
                "name": "ç¼ºå¤±å­—æ®µ",
                "data": {
                    "student_answer": {"question_type": "short_answer"},
                    "grading_criteria": {"max_score": 100}
                },
                "expected_handled": True
            },
            {
                "name": "æ— æ•ˆåˆ†æ•°",
                "data": {
                    "student_answer": {"content": "æµ‹è¯•", "question_type": "short_answer"},
                    "grading_criteria": {"max_score": -1}
                },
                "expected_handled": True
            }
        ]
        
        properly_handled = 0
        
        for scenario in error_scenarios:
            try:
                response = requests.post(
                    f"{self.base_url}/api/ai-grading/grade-single",
                    json=scenario["data"],
                    timeout=10
                )
                
                # æ£€æŸ¥æ˜¯å¦æ­£ç¡®å¤„ç†é”™è¯¯
                if response.status_code in [200, 400, 422]:  # æ­£å¸¸æˆ–é¢„æœŸçš„é”™è¯¯å“åº”
                    if response.status_code == 200:
                        # æˆåŠŸå“åº”ï¼Œæ£€æŸ¥ç»“æœæ˜¯å¦åˆç†
                        result = response.json()
                        if result.get('data', {}).get('score', -1) >= 0:
                            properly_handled += 1
                    else:
                        # é”™è¯¯å“åº”ï¼Œè¡¨ç¤ºæ­£ç¡®å¤„ç†äº†é”™è¯¯è¾“å…¥
                        properly_handled += 1
                        
                logger.info(f"   âœ… {scenario['name']}: æ­£ç¡®å¤„ç† ({response.status_code})")
                
            except Exception as e:
                logger.info(f"   âœ… {scenario['name']}: å¼‚å¸¸å¤„ç† ({e})")
                properly_handled += 1  # å¼‚å¸¸ä¹Ÿè¡¨ç¤ºé”™è¯¯è¢«å¤„ç†
        
        handling_rate = properly_handled / len(error_scenarios)
        
        logger.info(f"   ğŸ“Š é”™è¯¯å¤„ç†ç‡: {properly_handled}/{len(error_scenarios)} ({handling_rate*100:.1f}%)")
        
        error_handling_result = TestResult(
            scenario_id="error_handling",
            status="PASS" if handling_rate >= 0.8 else "FAIL",
            execution_time=5.0,
            details={
                "scenarios_tested": len(error_scenarios),
                "properly_handled": properly_handled,
                "handling_rate": handling_rate
            },
            timestamp=datetime.now()
        )
        self.test_results.append(error_handling_result)
    
    async def _generate_test_report(self, total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        logger.info("ğŸ“‹ ç”Ÿæˆé›†æˆæµ‹è¯•æŠ¥å‘Š...")
        
        # ç»Ÿè®¡æµ‹è¯•ç»“æœ
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results if r.status == "PASS"])
        failed_tests = len([r for r in self.test_results if r.status == "FAIL"])
        error_tests = len([r for r in self.test_results if r.status == "ERROR"])
        warning_tests = len([r for r in self.test_results if r.status == "WARN"])
        
        success_rate = passed_tests / total_tests if total_tests > 0 else 0
        
        # æ€§èƒ½ç»Ÿè®¡
        avg_execution_time = np.mean([r.execution_time for r in self.test_results])
        max_execution_time = max([r.execution_time for r in self.test_results]) if self.test_results else 0
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "test_summary": {
                "execution_time": datetime.now().isoformat(),
                "total_duration_seconds": total_time,
                "total_tests": total_tests,
                "passed": passed_tests,
                "failed": failed_tests,
                "errors": error_tests,
                "warnings": warning_tests,
                "success_rate": success_rate
            },
            "performance_metrics": {
                "avg_test_execution_time": avg_execution_time,
                "max_test_execution_time": max_execution_time,
                "tests_per_second": total_tests / total_time if total_time > 0 else 0
            },
            "test_categories": {
                "system_health": len([r for r in self.test_results if "health" in r.scenario_id]),
                "end_to_end": len([r for r in self.test_results if "workflow" in r.scenario_id]),
                "performance": len([r for r in self.test_results if "performance" in r.scenario_id or "stress" in r.scenario_id]),
                "security": len([r for r in self.test_results if "security" in r.scenario_id or "injection" in r.scenario_id or "xss" in r.scenario_id or "auth" in r.scenario_id]),
                "compatibility": len([r for r in self.test_results if "compatibility" in r.scenario_id]),
                "user_scenarios": len([r for r in self.test_results if "scenario" in r.scenario_id and "workflow" not in r.scenario_id]),
                "data_consistency": len([r for r in self.test_results if "consistency" in r.scenario_id]),
                "failure_recovery": len([r for r in self.test_results if "recovery" in r.scenario_id or "error" in r.scenario_id])
            },
            "detailed_results": [asdict(result) for result in self.test_results],
            "recommendations": self._generate_recommendations()
        }
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = self.test_data_path / f"integration_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        # æ‰“å°æ‘˜è¦
        logger.info("ğŸ“Š é›†æˆæµ‹è¯•ç»“æœæ‘˜è¦:")
        logger.info(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        logger.info(f"   é€šè¿‡: {passed_tests} ({passed_tests/total_tests*100:.1f}%)")
        logger.info(f"   å¤±è´¥: {failed_tests} ({failed_tests/total_tests*100:.1f}%)")
        logger.info(f"   é”™è¯¯: {error_tests} ({error_tests/total_tests*100:.1f}%)")
        logger.info(f"   è­¦å‘Š: {warning_tests} ({warning_tests/total_tests*100:.1f}%)")
        logger.info(f"   æˆåŠŸç‡: {success_rate*100:.1f}%")
        logger.info(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
        logger.info(f"   æŠ¥å‘Šæ–‡ä»¶: {report_file}")
        
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åˆ†æå¤±è´¥çš„æµ‹è¯•
        failed_tests = [r for r in self.test_results if r.status in ["FAIL", "ERROR"]]
        
        if failed_tests:
            recommendations.append(f"å‘ç° {len(failed_tests)} ä¸ªå¤±è´¥æµ‹è¯•ï¼Œéœ€è¦ä¼˜å…ˆä¿®å¤")
        
        # æ€§èƒ½å»ºè®®
        slow_tests = [r for r in self.test_results if r.execution_time > 10]
        if slow_tests:
            recommendations.append(f"æœ‰ {len(slow_tests)} ä¸ªæµ‹è¯•æ‰§è¡Œæ—¶é—´è¶…è¿‡10ç§’ï¼Œå»ºè®®ä¼˜åŒ–æ€§èƒ½")
        
        # å®‰å…¨å»ºè®®
        security_failures = [r for r in self.test_results if r.status == "FAIL" and any(keyword in r.scenario_id for keyword in ["security", "injection", "xss", "auth"])]
        if security_failures:
            recommendations.append("å‘ç°å®‰å…¨æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ç«‹å³ä¿®å¤å®‰å…¨æ¼æ´")
        
        # ä¸€è‡´æ€§å»ºè®®
        consistency_failures = [r for r in self.test_results if r.status == "FAIL" and "consistency" in r.scenario_id]
        if consistency_failures:
            recommendations.append("æ•°æ®ä¸€è‡´æ€§æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥å¹¶å‘å¤„ç†é€»è¾‘")
        
        # å¦‚æœæ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡
        if not failed_tests:
            recommendations.append("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œç³»ç»ŸçŠ¶æ€è‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘ç”Ÿäº§éƒ¨ç½²")
        
        return recommendations

async def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶å®ä¾‹
    test_suite = SystemIntegrationTestSuite()
    
    try:
        # è¿è¡Œå®Œæ•´çš„é›†æˆæµ‹è¯•
        report = await test_suite.run_full_integration_test()
        
        # åˆ¤æ–­æµ‹è¯•ç»“æœ
        success_rate = report["test_summary"]["success_rate"]
        
        if success_rate >= 0.9:
            logger.info("ğŸ‰ é›†æˆæµ‹è¯•å®Œå…¨é€šè¿‡ï¼ç³»ç»Ÿå¯ä»¥è¿›å…¥ç”Ÿäº§ç¯å¢ƒã€‚")
            return 0
        elif success_rate >= 0.8:
            logger.warning("âš ï¸ é›†æˆæµ‹è¯•å¤§éƒ¨åˆ†é€šè¿‡ï¼Œä½†æœ‰ä¸€äº›é—®é¢˜éœ€è¦è§£å†³ã€‚")
            return 1
        else:
            logger.error("âŒ é›†æˆæµ‹è¯•å¤±è´¥è¾ƒå¤šï¼Œéœ€è¦ä¿®å¤åå†æ¬¡æµ‹è¯•ã€‚")
            return 2
            
    except Exception as e:
        logger.error(f"é›†æˆæµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return 3

if __name__ == "__main__":
    exit_code = asyncio.run(main())