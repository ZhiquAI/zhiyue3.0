#!/usr/bin/env python3
"""
æ™ºèƒ½è¯„åˆ†ç³»ç»Ÿæµ‹è¯•è„šæœ¬
æµ‹è¯• grade_single_answer_sheet æ–¹æ³•å’Œè¯„åˆ†ç»“æœæ•°æ®ç»“æ„
"""

import os
import asyncio
import json
from datetime import datetime

# è®¾ç½®æ¨¡æ‹Ÿçš„ API key ç”¨äºæµ‹è¯•
os.environ['GEMINI_API_KEY'] = 'test_api_key_for_demo'
from services.gemini_service import GeminiService
from models.grading_models import (
    GradingResult, ObjectiveQuestionResult, SubjectiveQuestionResult,
    QualityAssessment, QuestionType, QualityLevel, ExamGradingConfig
)

def create_mock_ocr_result():
    """åˆ›å»ºæ¨¡æ‹Ÿçš„OCRè¯†åˆ«ç»“æœ"""
    return {
        "student_info": {
            "student_id": "2024001",
            "student_name": "å¼ ä¸‰",
            "class_name": "é«˜ä¸‰(1)ç­"
        },
        "objective_answers": {
            "1": "A",
            "2": "B",
            "3": "C",
            "4": "D",
            "5": "A"
        },
        "subjective_answers": {
            "6": "ç‰›é¡¿ç¬¬ä¸€å®šå¾‹è¡¨æ˜ï¼Œç‰©ä½“åœ¨ä¸å—å¤–åŠ›ä½œç”¨æ—¶ï¼Œå°†ä¿æŒé™æ­¢æˆ–åŒ€é€Ÿç›´çº¿è¿åŠ¨çŠ¶æ€ã€‚",
            "7": "æ ¹æ®åŠ¨é‡å®ˆæ’å®šå¾‹ï¼Œç³»ç»Ÿæ€»åŠ¨é‡åœ¨ç¢°æ’å‰åä¿æŒä¸å˜ã€‚è®¾ä¸¤çƒè´¨é‡åˆ†åˆ«ä¸ºm1å’Œm2ï¼Œç¢°æ’å‰é€Ÿåº¦ä¸ºv1å’Œv2ï¼Œç¢°æ’åé€Ÿåº¦ä¸ºv1'å’Œv2'ï¼Œåˆ™æœ‰ï¼šm1*v1 + m2*v2 = m1*v1' + m2*v2'"
        },
        "confidence": 0.85,
        "quality_issues": []
    }

def create_mock_exam_config():
    """åˆ›å»ºæ¨¡æ‹Ÿçš„è€ƒè¯•é…ç½®"""
    return {
        "exam_id": "physics_test_2024",
        "subject": "ç‰©ç†",
        "total_score": 100.0,
        "objective_answers": {
            "1": "A",
            "2": "B",
            "3": "C",
            "4": "D",
            "5": "A"
        },
        "objective_scores": {
            "1": 4.0,
            "2": 4.0,
            "3": 4.0,
            "4": 4.0,
            "5": 4.0
        },
        "subjective_questions": {
            "6": {
                "question_text": "è¯·ç®€è¿°ç‰›é¡¿ç¬¬ä¸€å®šå¾‹çš„å†…å®¹",
                "max_score": 10.0,
                "key_points": ["ç‰©ä½“", "ä¸å—å¤–åŠ›", "é™æ­¢æˆ–åŒ€é€Ÿç›´çº¿è¿åŠ¨"],
                "sample_answer": "ç‰›é¡¿ç¬¬ä¸€å®šå¾‹ï¼šç‰©ä½“åœ¨ä¸å—å¤–åŠ›ä½œç”¨æ—¶ï¼Œå°†ä¿æŒé™æ­¢æˆ–åŒ€é€Ÿç›´çº¿è¿åŠ¨çŠ¶æ€ã€‚"
            },
            "7": {
                "question_text": "ä¸¤ä¸ªå°çƒå‘ç”Ÿå¼¹æ€§ç¢°æ’ï¼Œè¯·ç”¨åŠ¨é‡å®ˆæ’å®šå¾‹åˆ†æ",
                "max_score": 20.0,
                "key_points": ["åŠ¨é‡å®ˆæ’", "ç¢°æ’å‰å", "è´¨é‡", "é€Ÿåº¦", "å…¬å¼"],
                "sample_answer": "æ ¹æ®åŠ¨é‡å®ˆæ’å®šå¾‹ï¼Œç³»ç»Ÿæ€»åŠ¨é‡åœ¨ç¢°æ’å‰åä¿æŒä¸å˜ã€‚"
            }
        }
    }

async def test_grading_system():
    """æµ‹è¯•æ™ºèƒ½è¯„åˆ†ç³»ç»Ÿ"""
    print("=== æ™ºèƒ½è¯„åˆ†ç³»ç»Ÿæµ‹è¯• ===")
    print(f"æµ‹è¯•æ—¶é—´: {datetime.now()}")
    print()
    
    try:
        # 1. åˆ›å»º GeminiService å®ä¾‹
        print("1. åˆå§‹åŒ– Gemini è¯„åˆ†æœåŠ¡...")
        gemini_service = GeminiService()
        print("âœ“ Gemini æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        print()
        
        # 2. å‡†å¤‡æµ‹è¯•æ•°æ®
        print("2. å‡†å¤‡æµ‹è¯•æ•°æ®...")
        ocr_result = create_mock_ocr_result()
        exam_config = create_mock_exam_config()
        
        print(f"âœ“ OCRç»“æœ: å­¦ç”Ÿ {ocr_result['student_info']['student_name']} ({ocr_result['student_info']['student_id']})")
        print(f"âœ“ å®¢è§‚é¢˜æ•°é‡: {len(ocr_result['objective_answers'])}")
        print(f"âœ“ ä¸»è§‚é¢˜æ•°é‡: {len(ocr_result['subjective_answers'])}")
        print()
        
        # 3. æ‰§è¡Œè¯„åˆ†
        print("3. æ‰§è¡Œæ™ºèƒ½è¯„åˆ†...")
        grading_result: GradingResult = await gemini_service.grade_answer_sheet(
            ocr_result=ocr_result,
            exam_config=exam_config
        )
        print("âœ“ è¯„åˆ†å®Œæˆ")
        print()
        
        # 4. å±•ç¤ºè¯„åˆ†ç»“æœ
        print("4. è¯„åˆ†ç»“æœåˆ†æ:")
        print(f"æ€»åˆ†: {grading_result.total_score}")
        print(f"è¯„åˆ†å¼•æ“: {grading_result.grading_engine}")
        print(f"è¯„åˆ†æ—¶é—´: {grading_result.graded_at}")
        print(f"å¤„ç†æ—¶é—´: {grading_result.processing_time:.2f}ç§’")
        print()
        
        # å®¢è§‚é¢˜ç»“æœ
        print("å®¢è§‚é¢˜è¯„åˆ†ç»“æœ:")
        objective_total = 0
        for result in grading_result.objective_results:
            status = "âœ“" if result.is_correct else "âœ—"
            print(f"  é¢˜ç›®{result.question_number}: {status} {result.student_answer} (å¾—åˆ†: {result.earned_score}/{result.max_score})")
            objective_total += result.earned_score
        print(f"å®¢è§‚é¢˜æ€»åˆ†: {objective_total}")
        print()
        
        # ä¸»è§‚é¢˜ç»“æœ
        print("ä¸»è§‚é¢˜è¯„åˆ†ç»“æœ:")
        subjective_total = 0
        for result in grading_result.subjective_results:
            print(f"  é¢˜ç›®{result.question_number}: {result.earned_score}/{result.max_score}åˆ†")
            print(f"    åé¦ˆ: {result.feedback[:100]}..." if len(result.feedback) > 100 else f"    åé¦ˆ: {result.feedback}")
            print(f"    ç½®ä¿¡åº¦: {result.confidence:.2f}")
            subjective_total += result.earned_score
        print(f"ä¸»è§‚é¢˜æ€»åˆ†: {subjective_total}")
        print()
        
        # è´¨é‡è¯„ä¼°
        if grading_result.quality_assessment:
            qa = grading_result.quality_assessment
            print("è´¨é‡è¯„ä¼°:")
            print(f"  æ•´ä½“è´¨é‡: {qa.overall_quality.value}")
            print(f"  OCRç½®ä¿¡åº¦: {qa.ocr_confidence:.2f}")
            print(f"  è¯„åˆ†ç½®ä¿¡åº¦: {qa.grading_confidence:.2f}")
            print(f"  éœ€è¦äººå·¥å¤æ ¸: {'æ˜¯' if qa.needs_human_review else 'å¦'}")
            
            if qa.issues:
                print("  å‘ç°çš„é—®é¢˜:")
                for issue in qa.issues:
                    print(f"    - {issue}")
            
            if qa.recommendations:
                print("  å»ºè®®:")
                for rec in qa.recommendations:
                    print(f"    - {rec}")
        print()
        
        # 5. æµ‹è¯•æ•°æ®ç»“æ„åºåˆ—åŒ–
        print("5. æµ‹è¯•æ•°æ®ç»“æ„åºåˆ—åŒ–...")
        try:
            # ä½¿ç”¨å†…ç½®çš„ to_dict æ–¹æ³•
            result_dict = grading_result.to_dict()
            
            # æµ‹è¯•JSONåºåˆ—åŒ–
            json_str = json.dumps(result_dict, ensure_ascii=False, indent=2)
            print("âœ“ æ•°æ®ç»“æ„åºåˆ—åŒ–æˆåŠŸ")
            print(f"âœ“ JSONå¤§å°: {len(json_str)} å­—ç¬¦")
        except Exception as e:
            print(f"âœ— åºåˆ—åŒ–å¤±è´¥: {e}")
        print()
        
        print("=== æµ‹è¯•å®Œæˆ ===")
        print("âœ“ æ‰€æœ‰åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        
        return True
        
    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_data_structures():
    """æµ‹è¯•è¯„åˆ†æ•°æ®ç»“æ„"""
    print("\n=== æ•°æ®ç»“æ„æµ‹è¯• ===")
    
    try:
        # æµ‹è¯• ObjectiveQuestionResult
        obj_result = ObjectiveQuestionResult(
            question_number="1",
            question_type=QuestionType.CHOICE,
            student_answer="A",
            standard_answer="A",
            is_correct=True,
            earned_score=4.0,
            max_score=4.0,
            confidence=1.0
        )
        print("âœ“ ObjectiveQuestionResult åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯• SubjectiveQuestionResult
        subj_result = SubjectiveQuestionResult(
            question_number="6",
            question_type=QuestionType.SHORT_ANSWER,
            student_answer="æµ‹è¯•ç­”æ¡ˆ",
            earned_score=8.0,
            max_score=10.0,
            feedback="ç­”æ¡ˆåŸºæœ¬æ­£ç¡®ï¼Œä½†ç¼ºå°‘éƒ¨åˆ†è¦ç‚¹",
            key_points_covered=["è¦ç‚¹1", "è¦ç‚¹2"],
            missing_points=["è¦ç‚¹3"],
            confidence=0.8
        )
        print("âœ“ SubjectiveQuestionResult åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯• QualityAssessment
        quality = QualityAssessment(
            overall_quality=QualityLevel.GOOD,
            ocr_confidence=0.85,
            grading_confidence=0.82,
            issues=["OCRè¯†åˆ«ç½®ä¿¡åº¦è¾ƒä½"],
            recommendations=["å»ºè®®äººå·¥å¤æ ¸OCRè¯†åˆ«ç»“æœ"],
            needs_human_review=False
        )
        print("âœ“ QualityAssessment åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯• GradingResult
        grading_result = GradingResult(
            answer_sheet_id="test_001",
            exam_id="test_exam",
            student_id="2024001",
            total_score=12.0,
            objective_score=4.0,
            subjective_total_score=8.0,
            max_possible_score=14.0,
            objective_results=[obj_result],
            subjective_results=[subj_result],
            quality_assessment=quality
        )
        print("âœ“ GradingResult åˆ›å»ºæˆåŠŸ")
        
        print("âœ“ æ‰€æœ‰æ•°æ®ç»“æ„æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— æ•°æ®ç»“æ„æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("æ™ºèƒ½è¯„åˆ†ç³»ç»Ÿé›†æˆæµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•æ•°æ®ç»“æ„
    struct_test = test_data_structures()
    
    # æµ‹è¯•è¯„åˆ†ç³»ç»Ÿ
    grading_test = asyncio.run(test_grading_system())
    
    print("\n=== æµ‹è¯•æ€»ç»“ ===")
    print(f"æ•°æ®ç»“æ„æµ‹è¯•: {'é€šè¿‡' if struct_test else 'å¤±è´¥'}")
    print(f"è¯„åˆ†ç³»ç»Ÿæµ‹è¯•: {'é€šè¿‡' if grading_test else 'å¤±è´¥'}")
    
    if struct_test and grading_test:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ™ºèƒ½è¯„åˆ†ç³»ç»Ÿå·²å°±ç»ªã€‚")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®ã€‚")