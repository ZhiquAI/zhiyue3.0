#!/usr/bin/env python3
"""
AIæ™ºèƒ½è¯„åˆ†å¼•æ“æ¼”ç¤ºè„šæœ¬ - Phase 3 Week 16
å±•ç¤ºå¤šæ¨¡æ€è¯„åˆ†å¼•æ“çš„æ ¸å¿ƒåŠŸèƒ½å’Œæ€§èƒ½
"""

import asyncio
import json
import time
import logging
from datetime import datetime
from typing import List, Dict, Any

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯¼å…¥è¯„åˆ†å¼•æ“ç»„ä»¶
try:
    from services.multimodal_grading_engine import (
        get_grading_engine,
        QuestionType,
        GradingMode,
        GradingCriteria,
        StudentAnswer,
        demo_grading
    )
except ImportError as e:
    logger.error(f"å¯¼å…¥è¯„åˆ†å¼•æ“å¤±è´¥: {e}")
    exit(1)


class AIGradingDemo:
    """AIè¯„åˆ†æ¼”ç¤ºç±»"""
    
    def __init__(self):
        self.engine = None
        self.demo_data = self._prepare_demo_data()
        
    async def initialize(self):
        """åˆå§‹åŒ–æ¼”ç¤ºç¯å¢ƒ"""
        logger.info("ğŸš€ åˆå§‹åŒ–AIæ™ºèƒ½è¯„åˆ†æ¼”ç¤ºç¯å¢ƒ...")
        
        try:
            self.engine = await get_grading_engine()
            logger.info("âœ… AIè¯„åˆ†å¼•æ“åˆå§‹åŒ–å®Œæˆ")
            return True
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _prepare_demo_data(self) -> Dict[str, Any]:
        """å‡†å¤‡æ¼”ç¤ºæ•°æ®"""
        return {
            "multiple_choice_demos": [
                {
                    "question": "ä»¥ä¸‹å“ªä¸ªæ˜¯å…‰åˆä½œç”¨çš„ä¸»è¦äº§ç‰©ï¼Ÿ",
                    "student_answer": "B",
                    "correct_answer": "B",
                    "options": {"A": "æ°´", "B": "æ°§æ°”", "C": "äºŒæ°§åŒ–ç¢³", "D": "æ°®æ°”"}
                },
                {
                    "question": "åœ°çƒçš„å«æ˜Ÿæ˜¯ä»€ä¹ˆï¼Ÿ",
                    "student_answer": "A",
                    "correct_answer": "A", 
                    "options": {"A": "æœˆçƒ", "B": "å¤ªé˜³", "C": "ç«æ˜Ÿ", "D": "é‡‘æ˜Ÿ"}
                }
            ],
            "short_answer_demos": [
                {
                    "question": "è¯·ç®€è¿°å…‰åˆä½œç”¨çš„è¿‡ç¨‹",
                    "student_answer": "å…‰åˆä½œç”¨æ˜¯æ¤ç‰©åˆ©ç”¨é˜³å…‰ã€æ°´å’ŒäºŒæ°§åŒ–ç¢³åˆ¶é€ æœ‰æœºç‰©çš„è¿‡ç¨‹ï¼ŒåŒæ—¶é‡Šæ”¾æ°§æ°”",
                    "standard_answer": "å…‰åˆä½œç”¨æ˜¯æ¤ç‰©åˆ©ç”¨é˜³å…‰ã€æ°´åˆ†å’ŒäºŒæ°§åŒ–ç¢³åˆæˆæœ‰æœºç‰©è´¨çš„ç”Ÿç‰©è¿‡ç¨‹ï¼Œäº§ç”Ÿæ°§æ°”ä½œä¸ºå‰¯äº§å“",
                    "keywords": ["å…‰åˆä½œç”¨", "é˜³å…‰", "æ°´", "äºŒæ°§åŒ–ç¢³", "æœ‰æœºç‰©", "æ°§æ°”"]
                },
                {
                    "question": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
                    "student_answer": "äººå·¥æ™ºèƒ½æ˜¯è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½è¡Œä¸ºçš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬å­¦ä¹ ã€æ¨ç†å’Œå†³ç­–",
                    "standard_answer": "äººå·¥æ™ºèƒ½æ˜¯ä½¿æœºå™¨èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æŠ€æœ¯é¢†åŸŸ",
                    "keywords": ["äººå·¥æ™ºèƒ½", "æœºå™¨", "æ™ºèƒ½", "å­¦ä¹ ", "æ¨ç†"]
                }
            ],
            "essay_demos": [
                {
                    "question": "è¯·ä»¥'æˆ‘çš„æ¢¦æƒ³'ä¸ºé¢˜å†™ä¸€ç¯‡çŸ­æ–‡",
                    "student_answer": """æˆ‘çš„æ¢¦æƒ³
æˆ‘çš„æ¢¦æƒ³æ˜¯æˆä¸ºä¸€åç§‘å­¦å®¶ã€‚ä»å°æˆ‘å°±å¯¹ç§‘å­¦å……æ»¡å¥½å¥‡ï¼Œå–œæ¬¢æ¢ç´¢è‡ªç„¶çš„å¥¥ç§˜ã€‚
æˆ‘å¸Œæœ›èƒ½å¤Ÿé€šè¿‡è‡ªå·±çš„ç ”ç©¶ï¼Œä¸ºäººç±»ç¤¾ä¼šåšå‡ºè´¡çŒ®ã€‚ç§‘å­¦å®¶å¯ä»¥å‘æ˜æ–°æŠ€æœ¯ï¼Œ
è§£å†³å®é™…é—®é¢˜ï¼Œè®©ä¸–ç•Œå˜å¾—æ›´ç¾å¥½ã€‚ä¸ºäº†å®ç°è¿™ä¸ªæ¢¦æƒ³ï¼Œæˆ‘è¦åŠªåŠ›å­¦ä¹ ï¼Œ
æŒæ¡æ‰å®çš„ç§‘å­¦çŸ¥è¯†ï¼ŒåŸ¹å…»åˆ›æ–°æ€ç»´å’Œå®éªŒèƒ½åŠ›ã€‚
è™½ç„¶é“è·¯å¯èƒ½å……æ»¡æŒ‘æˆ˜ï¼Œä½†æˆ‘ç›¸ä¿¡åªè¦åšæŒä¸æ‡ˆï¼Œæ¢¦æƒ³ä¸€å®šèƒ½å®ç°ã€‚""",
                    "criteria": {
                        "content": "å†…å®¹ä¸°å¯Œï¼Œä¸»é¢˜æ˜ç¡®",
                        "structure": "ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘åˆç†", 
                        "language": "è¯­è¨€æµç•…ï¼Œè¡¨è¾¾å‡†ç¡®",
                        "creativity": "æœ‰ä¸€å®šçš„åˆ›æ–°æ€§"
                    }
                }
            ],
            "calculation_demos": [
                {
                    "question": "è®¡ç®—ï¼š3x + 5 = 14ï¼Œæ±‚xçš„å€¼",
                    "student_answer": "x = 3",
                    "correct_answers": ["3", "x=3", "x = 3"]
                },
                {
                    "question": "ä¸€ä¸ªåœ†çš„åŠå¾„æ˜¯5cmï¼Œæ±‚å…¶é¢ç§¯ï¼ˆÏ€å–3.14ï¼‰",
                    "student_answer": "é¢ç§¯ = 78.5å¹³æ–¹å˜ç±³",
                    "correct_answers": ["78.5", "78.5å¹³æ–¹å˜ç±³", "78.5cmÂ²"]
                }
            ]
        }
    
    async def demo_multiple_choice_grading(self):
        """æ¼”ç¤ºé€‰æ‹©é¢˜è¯„åˆ†"""
        logger.info("\nğŸ“ === é€‰æ‹©é¢˜è¯„åˆ†æ¼”ç¤º ===")
        
        for i, demo in enumerate(self.demo_data["multiple_choice_demos"], 1):
            logger.info(f"\né€‰æ‹©é¢˜ {i}: {demo['question']}")
            
            # åˆ›å»ºå­¦ç”Ÿç­”æ¡ˆ
            answer = StudentAnswer(
                question_id=f"MC{i:03d}",
                question_type=QuestionType.MULTIPLE_CHOICE,
                raw_text=demo["student_answer"],
                processed_text=demo["student_answer"]
            )
            
            # åˆ›å»ºè¯„åˆ†æ ‡å‡†
            criteria = GradingCriteria(
                max_score=5.0,
                rubric={"type": "é€‰æ‹©é¢˜", "options": demo["options"]},
                acceptable_answers=[demo["correct_answer"]]
            )
            
            # æ‰§è¡Œè¯„åˆ†
            start_time = time.time()
            result = await self.engine.grade_answer(answer, criteria, GradingMode.AUTOMATIC)
            processing_time = time.time() - start_time
            
            # æ˜¾ç¤ºç»“æœ
            logger.info(f"å­¦ç”Ÿé€‰æ‹©: {demo['student_answer']}")
            logger.info(f"æ­£ç¡®ç­”æ¡ˆ: {demo['correct_answer']}")
            logger.info(f"è¯„åˆ†ç»“æœ: {result.score}/{result.max_score}")
            logger.info(f"AIç½®ä¿¡åº¦: {result.confidence:.2f}")
            logger.info(f"å¤„ç†æ—¶é—´: {processing_time:.3f}ç§’")
            logger.info(f"AIæ¨ç†: {result.ai_reasoning}")
    
    async def demo_short_answer_grading(self):
        """æ¼”ç¤ºç®€ç­”é¢˜è¯„åˆ†"""
        logger.info("\nâœï¸ === ç®€ç­”é¢˜è¯„åˆ†æ¼”ç¤º ===")
        
        for i, demo in enumerate(self.demo_data["short_answer_demos"], 1):
            logger.info(f"\nç®€ç­”é¢˜ {i}: {demo['question']}")
            
            # åˆ›å»ºå­¦ç”Ÿç­”æ¡ˆ
            answer = StudentAnswer(
                question_id=f"SA{i:03d}",
                question_type=QuestionType.SHORT_ANSWER,
                raw_text=demo["student_answer"],
                processed_text=demo["student_answer"]
            )
            
            # åˆ›å»ºè¯„åˆ†æ ‡å‡†
            criteria = GradingCriteria(
                max_score=10.0,
                rubric={"type": "ç®€ç­”é¢˜", "evaluation": "è¯­ä¹‰ç›¸ä¼¼åº¦+å…³é”®è¯"},
                keywords=demo["keywords"],
                acceptable_answers=[demo["standard_answer"]],
                bonus_rules={kw: 0.5 for kw in demo["keywords"]}
            )
            
            # æ‰§è¡Œè¯„åˆ†
            start_time = time.time()
            result = await self.engine.grade_answer(answer, criteria, GradingMode.AUTOMATIC)
            processing_time = time.time() - start_time
            
            # æ˜¾ç¤ºç»“æœ
            logger.info(f"å­¦ç”Ÿç­”æ¡ˆ: {demo['student_answer']}")
            logger.info(f"æ ‡å‡†ç­”æ¡ˆ: {demo['standard_answer']}")
            logger.info(f"è¯„åˆ†ç»“æœ: {result.score:.1f}/{result.max_score}")
            logger.info(f"AIç½®ä¿¡åº¦: {result.confidence:.2f}")
            logger.info(f"å¤„ç†æ—¶é—´: {processing_time:.3f}ç§’")
            logger.info(f"åŒ¹é…å…³é”®è¯: {result.detailed_feedback.get('matched_keywords', [])}")
            logger.info(f"è¯­ä¹‰ç›¸ä¼¼åº¦: {result.detailed_feedback.get('semantic_similarity', 0):.2f}")
            logger.info(f"AIæ¨ç†: {result.ai_reasoning}")
            
            if result.review_required:
                logger.warning("âš ï¸ æ­¤ç­”æ¡ˆéœ€è¦äººå·¥å¤æ ¸")
    
    async def demo_essay_grading(self):
        """æ¼”ç¤ºä½œæ–‡è¯„åˆ†"""
        logger.info("\nğŸ“– === ä½œæ–‡è¯„åˆ†æ¼”ç¤º ===")
        
        for i, demo in enumerate(self.demo_data["essay_demos"], 1):
            logger.info(f"\nä½œæ–‡é¢˜ {i}: {demo['question']}")
            
            # åˆ›å»ºå­¦ç”Ÿç­”æ¡ˆ
            answer = StudentAnswer(
                question_id=f"ES{i:03d}",
                question_type=QuestionType.ESSAY,
                raw_text=demo["student_answer"],
                processed_text=demo["student_answer"]
            )
            
            # åˆ›å»ºè¯„åˆ†æ ‡å‡†
            criteria = GradingCriteria(
                max_score=20.0,
                rubric=demo["criteria"]
            )
            
            # æ‰§è¡Œè¯„åˆ†
            start_time = time.time()
            result = await self.engine.grade_answer(answer, criteria, GradingMode.ASSISTED)
            processing_time = time.time() - start_time
            
            # æ˜¾ç¤ºç»“æœ
            logger.info(f"ä½œæ–‡å†…å®¹é•¿åº¦: {len(demo['student_answer'])}å­—")
            logger.info(f"è¯„åˆ†ç»“æœ: {result.score:.1f}/{result.max_score}")
            logger.info(f"AIç½®ä¿¡åº¦: {result.confidence:.2f}")
            logger.info(f"å¤„ç†æ—¶é—´: {processing_time:.3f}ç§’")
            
            # æ˜¾ç¤ºå„ç»´åº¦å¾—åˆ†
            dimension_scores = result.detailed_feedback.get('dimension_scores', {})
            for dimension, score in dimension_scores.items():
                logger.info(f"  {dimension}: {score:.2f}")
            
            logger.info(f"AIæ¨ç†: {result.ai_reasoning}")
            logger.info(f"æ”¹è¿›å»ºè®®: {', '.join(result.suggestions)}")
            
            if result.review_required:
                logger.warning("âš ï¸ æ­¤ä½œæ–‡éœ€è¦äººå·¥å¤æ ¸")
    
    async def demo_calculation_grading(self):
        """æ¼”ç¤ºè®¡ç®—é¢˜è¯„åˆ†"""
        logger.info("\nğŸ”¢ === è®¡ç®—é¢˜è¯„åˆ†æ¼”ç¤º ===")
        
        for i, demo in enumerate(self.demo_data["calculation_demos"], 1):
            logger.info(f"\nè®¡ç®—é¢˜ {i}: {demo['question']}")
            
            # åˆ›å»ºå­¦ç”Ÿç­”æ¡ˆ
            answer = StudentAnswer(
                question_id=f"CA{i:03d}",
                question_type=QuestionType.CALCULATION,
                raw_text=demo["student_answer"],
                processed_text=demo["student_answer"]
            )
            
            # åˆ›å»ºè¯„åˆ†æ ‡å‡†
            criteria = GradingCriteria(
                max_score=8.0,
                rubric={"type": "è®¡ç®—é¢˜", "evaluation": "æ•°å€¼åŒ¹é…"},
                acceptable_answers=demo["correct_answers"]
            )
            
            # æ‰§è¡Œè¯„åˆ†
            start_time = time.time()
            result = await self.engine.grade_answer(answer, criteria, GradingMode.AUTOMATIC)
            processing_time = time.time() - start_time
            
            # æ˜¾ç¤ºç»“æœ
            logger.info(f"å­¦ç”Ÿç­”æ¡ˆ: {demo['student_answer']}")
            logger.info(f"æ ‡å‡†ç­”æ¡ˆ: {', '.join(demo['correct_answers'])}")
            logger.info(f"è¯„åˆ†ç»“æœ: {result.score:.1f}/{result.max_score}")
            logger.info(f"AIç½®ä¿¡åº¦: {result.confidence:.2f}")
            logger.info(f"å¤„ç†æ—¶é—´: {processing_time:.3f}ç§’")
            logger.info(f"AIæ¨ç†: {result.ai_reasoning}")
    
    async def demo_batch_grading(self):
        """æ¼”ç¤ºæ‰¹é‡è¯„åˆ†"""
        logger.info("\nğŸš€ === æ‰¹é‡è¯„åˆ†æ¼”ç¤º ===")
        
        # å‡†å¤‡æ‰¹é‡æ•°æ®
        answers = []
        criteria_list = []
        
        # æ·»åŠ å¤šç§é¢˜å‹çš„ç­”æ¡ˆ
        for demo in self.demo_data["multiple_choice_demos"]:
            answers.append(StudentAnswer(
                question_id=f"BATCH_MC_{len(answers)+1:03d}",
                question_type=QuestionType.MULTIPLE_CHOICE,
                raw_text=demo["student_answer"],
                processed_text=demo["student_answer"]
            ))
            criteria_list.append(GradingCriteria(
                max_score=5.0,
                acceptable_answers=[demo["correct_answer"]]
            ))
        
        for demo in self.demo_data["short_answer_demos"]:
            answers.append(StudentAnswer(
                question_id=f"BATCH_SA_{len(answers)+1:03d}",
                question_type=QuestionType.SHORT_ANSWER,
                raw_text=demo["student_answer"],
                processed_text=demo["student_answer"]
            ))
            criteria_list.append(GradingCriteria(
                max_score=10.0,
                keywords=demo["keywords"],
                acceptable_answers=[demo["standard_answer"]]
            ))
        
        logger.info(f"å‡†å¤‡æ‰¹é‡è¯„åˆ† {len(answers)} é“é¢˜ç›®...")
        
        # æ‰§è¡Œæ‰¹é‡è¯„åˆ†
        start_time = time.time()
        results = await self.engine.batch_grade(answers, criteria_list, GradingMode.AUTOMATIC)
        total_time = time.time() - start_time
        
        # ç»Ÿè®¡ç»“æœ
        total_score = sum(r.score for r in results)
        total_possible = sum(r.max_score for r in results)
        avg_confidence = sum(r.confidence for r in results) / len(results)
        review_count = sum(1 for r in results if r.review_required)
        
        logger.info(f"\nğŸ“Š æ‰¹é‡è¯„åˆ†ç»Ÿè®¡:")
        logger.info(f"æ€»é¢˜æ•°: {len(results)}")
        logger.info(f"æ€»å¾—åˆ†: {total_score:.1f}/{total_possible}")
        logger.info(f"å¹³å‡åˆ†: {(total_score/total_possible)*100:.1f}%")
        logger.info(f"å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.2f}")
        logger.info(f"éœ€å¤æ ¸æ•°é‡: {review_count}")
        logger.info(f"æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
        logger.info(f"å¹³å‡æ¯é¢˜ç”¨æ—¶: {total_time/len(results):.3f}ç§’")
        
        # æ˜¾ç¤ºé¢˜å‹åˆ†å¸ƒ
        type_stats = {}
        for answer, result in zip(answers, results):
            qt = answer.question_type.value
            if qt not in type_stats:
                type_stats[qt] = {"count": 0, "total_score": 0, "total_possible": 0}
            type_stats[qt]["count"] += 1
            type_stats[qt]["total_score"] += result.score
            type_stats[qt]["total_possible"] += result.max_score
        
        logger.info(f"\nğŸ“ˆ é¢˜å‹ç»Ÿè®¡:")
        for question_type, stats in type_stats.items():
            accuracy = (stats["total_score"] / stats["total_possible"]) * 100
            logger.info(f"  {question_type}: {stats['count']}é¢˜, å‡†ç¡®ç‡: {accuracy:.1f}%")
    
    async def demo_performance_metrics(self):
        """æ¼”ç¤ºæ€§èƒ½æŒ‡æ ‡"""
        logger.info("\nâš¡ === æ€§èƒ½æŒ‡æ ‡æ¼”ç¤º ===")
        
        # è·å–å¼•æ“çŠ¶æ€
        status = self.engine.get_engine_status()
        
        logger.info(f"å¼•æ“çŠ¶æ€: {'å·²åˆå§‹åŒ–' if status['initialized'] else 'æœªåˆå§‹åŒ–'}")
        logger.info(f"åŠ è½½çš„æ¨¡å‹æ•°é‡: {len(status['models'])}")
        
        logger.info("\nğŸ¤– AIæ¨¡å‹ä¿¡æ¯:")
        for model_name, model_info in status["models"].items():
            logger.info(f"  {model_name}:")
            logger.info(f"    ç±»å‹: {model_info.get('type', 'Unknown')}")
            logger.info(f"    ç‰ˆæœ¬: {model_info.get('version', 'Unknown')}")
            logger.info(f"    è®¾å¤‡: {model_info.get('device', 'Unknown')}")
        
        logger.info(f"\næ”¯æŒçš„é¢˜å‹: {', '.join(status['supported_question_types'])}")
        logger.info(f"æ”¯æŒçš„è¯„åˆ†æ¨¡å¼: {', '.join(status['supported_grading_modes'])}")
        
        # æ€§èƒ½å‹æµ‹
        logger.info("\nğŸ”¥ === æ€§èƒ½å‹æµ‹ ===")
        test_answer = StudentAnswer(
            question_id="PERF_TEST",
            question_type=QuestionType.SHORT_ANSWER,
            raw_text="è¿™æ˜¯ä¸€ä¸ªæ€§èƒ½æµ‹è¯•ç­”æ¡ˆ",
            processed_text="è¿™æ˜¯ä¸€ä¸ªæ€§èƒ½æµ‹è¯•ç­”æ¡ˆ"
        )
        
        test_criteria = GradingCriteria(
            max_score=10.0,
            acceptable_answers=["è¿™æ˜¯æ ‡å‡†ç­”æ¡ˆ"]
        )
        
        # è¿ç»­è¯„åˆ†æµ‹è¯•
        test_count = 10
        times = []
        
        for i in range(test_count):
            start = time.time()
            await self.engine.grade_answer(test_answer, test_criteria, GradingMode.AUTOMATIC)
            times.append(time.time() - start)
        
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        
        logger.info(f"è¿ç»­è¯„åˆ†æµ‹è¯• ({test_count}æ¬¡):")
        logger.info(f"  å¹³å‡æ—¶é—´: {avg_time:.3f}ç§’")
        logger.info(f"  æœ€å¿«æ—¶é—´: {min_time:.3f}ç§’")
        logger.info(f"  æœ€æ…¢æ—¶é—´: {max_time:.3f}ç§’")
        logger.info(f"  ç†è®ºQPS: {1/avg_time:.1f}")
    
    async def run_full_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        logger.info("ğŸ¯ å¼€å§‹AIæ™ºèƒ½è¯„åˆ†å¼•æ“å®Œæ•´æ¼”ç¤º")
        logger.info("=" * 60)
        
        # åˆå§‹åŒ–
        if not await self.initialize():
            return False
        
        try:
            # å„ç§é¢˜å‹æ¼”ç¤º
            await self.demo_multiple_choice_grading()
            await self.demo_short_answer_grading()
            await self.demo_essay_grading()
            await self.demo_calculation_grading()
            
            # æ‰¹é‡è¯„åˆ†æ¼”ç¤º
            await self.demo_batch_grading()
            
            # æ€§èƒ½æŒ‡æ ‡æ¼”ç¤º
            await self.demo_performance_metrics()
            
            logger.info("\nğŸ‰ === æ¼”ç¤ºå®Œæˆ ===")
            logger.info("AIæ™ºèƒ½è¯„åˆ†å¼•æ“å·²æˆåŠŸå±•ç¤ºæ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½!")
            logger.info("âœ… å¤šæ¨¡æ€è¯„åˆ†æ”¯æŒ: é€‰æ‹©é¢˜ã€ç®€ç­”é¢˜ã€ä½œæ–‡ã€è®¡ç®—é¢˜")
            logger.info("âœ… æ™ºèƒ½åˆ†æèƒ½åŠ›: è¯­ä¹‰ç›¸ä¼¼åº¦ã€å…³é”®è¯åŒ¹é…ã€ç»“æ„åˆ†æ")
            logger.info("âœ… æ‰¹é‡å¤„ç†èƒ½åŠ›: å¹¶è¡Œè¯„åˆ†ã€ç»Ÿè®¡åˆ†æã€æ€§èƒ½ä¼˜åŒ–")
            logger.info("âœ… è´¨é‡æ§åˆ¶æœºåˆ¶: ç½®ä¿¡åº¦è¯„ä¼°ã€å¼‚å¸¸æ£€æµ‹ã€äººå·¥å¤æ ¸")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ æ™ºé˜…AI 3.0 - Phase 3 Week 16")
    logger.info("å¤šæ¨¡æ€è¯„åˆ†å¼•æ“æ¼”ç¤ºç¨‹åº")
    logger.info("=" * 60)
    
    demo = AIGradingDemo()
    success = await demo.run_full_demo()
    
    if success:
        logger.info("\nğŸ¯ æ¼”ç¤ºç¨‹åºæ‰§è¡ŒæˆåŠŸ!")
        logger.info("Phase 3 Week 16 å¤šæ¨¡æ€è¯„åˆ†å¼•æ“å¼€å‘å®Œæˆ âœ…")
    else:
        logger.error("\nâŒ æ¼”ç¤ºç¨‹åºæ‰§è¡Œå¤±è´¥!")
        return 1
    
    return 0


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    exit_code = asyncio.run(main())